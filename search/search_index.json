{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Honeydew Documentation","text":"<p>Honeydew library is a collection of ETL functions. Feel free to use and have fun with your next ETL workflows!</p>"},{"location":"#supported-connectors","title":"Supported connectors:","text":"<ul> <li>MySQL</li> <li>GCP</li> <li>SSH</li> </ul>"},{"location":"#utility-functions","title":"Utility functions:","text":"<ul> <li>Utils</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To install honeydew, run the following command from the command line: Text Only<pre><code>pip install honeydew --upgrade\n</code></pre></p>"},{"location":"#repository","title":"Repository","text":"<p>Check out the source code here!  Cheers!  Poltak</p>"},{"location":"gcp/","title":"GCP Connector","text":""},{"location":"gcp/#honeydew.gcp.GcpConnector","title":"<code>GcpConnector</code>","text":"<p>Instantiate a GCP connector. Args:     credential_file (str): Credential json file     proxy (str): Proxy address</p> Source code in <code>honeydew/gcp.py</code> Python<pre><code>class GcpConnector:\n    \"\"\"\n    Instantiate a GCP connector.\n    Args:\n        credential_file (str): Credential json file\n        proxy (str): Proxy address\n    \"\"\"\n    def __init__(self, credential_file, proxy=''):\n        self.credential_file = credential_file\n        self.proxy = proxy\n        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credential_file\n        if proxy != '':\n            os.environ['HTTP_PROXY'] = proxy\n            os.environ['HTTPS_PROXY'] = proxy\n\n    def bq_query_to_dataframe(self, project_id, query, timeout=3600, method=1):\n        \"\"\"\n        Submit query to BigQuery and store result into pandas dataframe\n        Args:\n            project_id (str): Project ID\n            query (str): SQL query\n            timeout (int): Query timeout in seconds\n            method (int): API that will be used to query (1: google-cloud-bigquery, 2: pandas-gbq)\n\n        Returns:\n            result (dataframe)): Result in pandas dataframe\n        \"\"\"\n        df = pd.DataFrame()\n        bqclient = bigquery.Client(project=project_id)\n        query_job = bqclient.query(query)\n        if method == 2:\n            df = pd.read_gbq(query=query, project_id=project_id)\n        else:\n            rows = list(query_job.result(timeout=timeout))\n            df = pd.DataFrame(data=[list(x.values()) for x in rows], columns=list(rows[0].keys()))\n        return df\n\n    def bq_query_non_dql(self, project_id, query):\n        \"\"\"\n        Submit non Data Query Language (DQL) type of query to BigQuery. Example: CREATE, DROP, TRUNCATE, INSERT, UPDATE, DELETE\n        Args:\n            project_id (str): Project ID\n            query (str): SQL query\n\n        Returns:\n            result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result\n        \"\"\"\n        bqclient = bigquery.Client(project=project_id)\n        query_job = bqclient.query(query)\n        results = query_job.result()\n        return results\n\n    def bq_export_table_to_gcs(self, project_id, dataset_id, table_id, gcs_uri, format='CSV', delimiter=',', enable_compression=True, compression='GZIP', overwrite=True, region='northamerica-northeast1'):\n        \"\"\"\n        Export BigQuery table into Google Cloud Storage (GCS)\n        Args:\n            project_id (str): Project ID\n            table_id (str): Table ID\n            dataset_id (str): Dataset ID\n            gcs_uri (str): GCS URI as destination. Example: 'gs://my-bucket/my-dir/tickets-20220101-*.csv.gz'\n            format (str): File format (CSV, JSON, Avro, Parquet). Default: 'CSV'\n            delimiter (str): CSV delimiter character. Default: ','\n            enable_compression (boolean): Files will be compressed if the value is True. Default: True\n            compression (str): Compression format. Default: GZIP. Reference: https://cloud.google.com/bigquery/docs/exporting-data#export_formats_and_compression_types\n            overwrite (boolean): GCS URI destination will be overwritten if the value is True. Default: True\n            region (str): Region to run the process. Default: 'northamerica-northeast1'\n\n        Returns:\n            result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result\n        \"\"\"\n        bqclient = bigquery.Client(project=project_id)\n        dataset_ref = bigquery.DatasetReference(project_id, dataset_id)\n        table_ref = dataset_ref.table(table_id)\n        job_config = bigquery.job.ExtractJobConfig()\n        if enable_compression == True:\n            if compression == 'DEFLATE':\n                job_config.compression = bigquery.Compression.DEFLATE\n            if compression == 'SNAPPY':\n                job_config.compression = bigquery.Compression.SNAPPY\n            else:\n                job_config.compression = bigquery.Compression.GZIP\n\n        extract_job = bqclient.extract_table(table_ref, gcs_uri, location=region, job_config=job_config)\n        results = extract_job.result()\n        return results\n\n    def gcs_download_single_file(self, project_id, bucket_id, source_blob_path, destination_path):\n        \"\"\"\n        Download a single object from Google Cloud Storage (GCS)\n        Args:\n            project_id (str): Project ID\n            bucket_id (str): Bucket ID\n            source_blob_path (str): The path of source object. Example: 'gcs-directory/my-filename.txt'\n            destination_path (str): Local destination path. Example: '/my-directory/my-filename.txt'\n\n        Returns:\n            result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result\n        \"\"\"\n        gcs_client = storage.Client(project=project_id)\n        bucket = gcs_client.bucket(bucket_id)\n        blob = bucket.blob(source_blob_path)\n        blob.download_to_filename(destination_path)\n        results = 'OK'\n        return results\n\n    def gcs_download_objects_with_pattern(self, project_id, bucket_id, blob_prefix, destination_dir_path, printout=True):\n        \"\"\"\n        Download multiple objects which have same prefix pattern from Google Cloud Storage (GCS)\n        Args:\n            project_id (str): Project ID\n            bucket_id (str): Bucket ID\n            blob_prefix (str): The blob prefix pattern that wil be downloaded. Example: 'gcs-directory/tickets-20220101-'\n            destination_dir_path (str): Local destination directory path. Example: '/my-directory'\n            printout (boolean): File name will be displayed if this value is true. Default: True\n        \"\"\"\n\n        delimiter='/'\n        storage_client = storage.Client(project_id)\n        bucket=storage_client.get_bucket(bucket_id)\n        # List blobs iterate in folder \n        blobs=bucket.list_blobs(prefix=blob_prefix, delimiter=delimiter) # Excluding folder inside bucket\n        for blob in blobs:\n            if printout == True:\n                print(blob.name)\n            filename_raw = blob.name.split('/')\n            filename = filename_raw[len(filename_raw)-1]\n            destination_uri = '{}{}'.format(destination_dir_path, filename) \n            blob.download_to_filename(destination_uri)\n        results = 'OK'   \n        return results\n\n    def gcs_upload_single_file(self, project_id, bucket_id, local_file, destination_blob):\n        \"\"\"\n        Upload a single object from Google Cloud Storage (GCS)\n        Args:\n            project_id (str): Project ID\n            bucket_id (str): Bucket ID\n            local_file (str): Local file as source. Example: '/local-directory/my-filename.txt'\n            destination_blob (str): Destination blob in GCS bucket. Example: 'gcs-directory/my-filename.txt'\n\n        Returns:\n            result (str): It returns 'OK' when successful\n        \"\"\"\n        gcs_client = storage.Client(project=project_id)\n        bucket = gcs_client.bucket(bucket_id)\n        blob = bucket.blob(destination_blob)\n        blob.upload_from_filename(local_file)\n        results = 'OK'\n        return results            \n</code></pre>"},{"location":"gcp/#honeydew.gcp.GcpConnector.bq_export_table_to_gcs","title":"<code>bq_export_table_to_gcs(project_id, dataset_id, table_id, gcs_uri, format='CSV', delimiter=',', enable_compression=True, compression='GZIP', overwrite=True, region='northamerica-northeast1')</code>","text":"<p>Export BigQuery table into Google Cloud Storage (GCS) Args:     project_id (str): Project ID     table_id (str): Table ID     dataset_id (str): Dataset ID     gcs_uri (str): GCS URI as destination. Example: 'gs://my-bucket/my-dir/tickets-20220101-*.csv.gz'     format (str): File format (CSV, JSON, Avro, Parquet). Default: 'CSV'     delimiter (str): CSV delimiter character. Default: ','     enable_compression (boolean): Files will be compressed if the value is True. Default: True     compression (str): Compression format. Default: GZIP. Reference: https://cloud.google.com/bigquery/docs/exporting-data#export_formats_and_compression_types     overwrite (boolean): GCS URI destination will be overwritten if the value is True. Default: True     region (str): Region to run the process. Default: 'northamerica-northeast1'</p> <p>Returns:</p> Name Type Description <code>result</code> <code>result</code> <p>Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result</p> Source code in <code>honeydew/gcp.py</code> Python<pre><code>def bq_export_table_to_gcs(self, project_id, dataset_id, table_id, gcs_uri, format='CSV', delimiter=',', enable_compression=True, compression='GZIP', overwrite=True, region='northamerica-northeast1'):\n    \"\"\"\n    Export BigQuery table into Google Cloud Storage (GCS)\n    Args:\n        project_id (str): Project ID\n        table_id (str): Table ID\n        dataset_id (str): Dataset ID\n        gcs_uri (str): GCS URI as destination. Example: 'gs://my-bucket/my-dir/tickets-20220101-*.csv.gz'\n        format (str): File format (CSV, JSON, Avro, Parquet). Default: 'CSV'\n        delimiter (str): CSV delimiter character. Default: ','\n        enable_compression (boolean): Files will be compressed if the value is True. Default: True\n        compression (str): Compression format. Default: GZIP. Reference: https://cloud.google.com/bigquery/docs/exporting-data#export_formats_and_compression_types\n        overwrite (boolean): GCS URI destination will be overwritten if the value is True. Default: True\n        region (str): Region to run the process. Default: 'northamerica-northeast1'\n\n    Returns:\n        result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result\n    \"\"\"\n    bqclient = bigquery.Client(project=project_id)\n    dataset_ref = bigquery.DatasetReference(project_id, dataset_id)\n    table_ref = dataset_ref.table(table_id)\n    job_config = bigquery.job.ExtractJobConfig()\n    if enable_compression == True:\n        if compression == 'DEFLATE':\n            job_config.compression = bigquery.Compression.DEFLATE\n        if compression == 'SNAPPY':\n            job_config.compression = bigquery.Compression.SNAPPY\n        else:\n            job_config.compression = bigquery.Compression.GZIP\n\n    extract_job = bqclient.extract_table(table_ref, gcs_uri, location=region, job_config=job_config)\n    results = extract_job.result()\n    return results\n</code></pre>"},{"location":"gcp/#honeydew.gcp.GcpConnector.bq_query_non_dql","title":"<code>bq_query_non_dql(project_id, query)</code>","text":"<p>Submit non Data Query Language (DQL) type of query to BigQuery. Example: CREATE, DROP, TRUNCATE, INSERT, UPDATE, DELETE Args:     project_id (str): Project ID     query (str): SQL query</p> <p>Returns:</p> Name Type Description <code>result</code> <code>result</code> <p>Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result</p> Source code in <code>honeydew/gcp.py</code> Python<pre><code>def bq_query_non_dql(self, project_id, query):\n    \"\"\"\n    Submit non Data Query Language (DQL) type of query to BigQuery. Example: CREATE, DROP, TRUNCATE, INSERT, UPDATE, DELETE\n    Args:\n        project_id (str): Project ID\n        query (str): SQL query\n\n    Returns:\n        result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result\n    \"\"\"\n    bqclient = bigquery.Client(project=project_id)\n    query_job = bqclient.query(query)\n    results = query_job.result()\n    return results\n</code></pre>"},{"location":"gcp/#honeydew.gcp.GcpConnector.bq_query_to_dataframe","title":"<code>bq_query_to_dataframe(project_id, query, timeout=3600, method=1)</code>","text":"<p>Submit query to BigQuery and store result into pandas dataframe Args:     project_id (str): Project ID     query (str): SQL query     timeout (int): Query timeout in seconds     method (int): API that will be used to query (1: google-cloud-bigquery, 2: pandas-gbq)</p> <p>Returns:</p> Name Type Description <code>result</code> <code>dataframe)</code> <p>Result in pandas dataframe</p> Source code in <code>honeydew/gcp.py</code> Python<pre><code>def bq_query_to_dataframe(self, project_id, query, timeout=3600, method=1):\n    \"\"\"\n    Submit query to BigQuery and store result into pandas dataframe\n    Args:\n        project_id (str): Project ID\n        query (str): SQL query\n        timeout (int): Query timeout in seconds\n        method (int): API that will be used to query (1: google-cloud-bigquery, 2: pandas-gbq)\n\n    Returns:\n        result (dataframe)): Result in pandas dataframe\n    \"\"\"\n    df = pd.DataFrame()\n    bqclient = bigquery.Client(project=project_id)\n    query_job = bqclient.query(query)\n    if method == 2:\n        df = pd.read_gbq(query=query, project_id=project_id)\n    else:\n        rows = list(query_job.result(timeout=timeout))\n        df = pd.DataFrame(data=[list(x.values()) for x in rows], columns=list(rows[0].keys()))\n    return df\n</code></pre>"},{"location":"gcp/#honeydew.gcp.GcpConnector.gcs_download_objects_with_pattern","title":"<code>gcs_download_objects_with_pattern(project_id, bucket_id, blob_prefix, destination_dir_path, printout=True)</code>","text":"<p>Download multiple objects which have same prefix pattern from Google Cloud Storage (GCS) Args:     project_id (str): Project ID     bucket_id (str): Bucket ID     blob_prefix (str): The blob prefix pattern that wil be downloaded. Example: 'gcs-directory/tickets-20220101-'     destination_dir_path (str): Local destination directory path. Example: '/my-directory'     printout (boolean): File name will be displayed if this value is true. Default: True</p> Source code in <code>honeydew/gcp.py</code> Python<pre><code>def gcs_download_objects_with_pattern(self, project_id, bucket_id, blob_prefix, destination_dir_path, printout=True):\n    \"\"\"\n    Download multiple objects which have same prefix pattern from Google Cloud Storage (GCS)\n    Args:\n        project_id (str): Project ID\n        bucket_id (str): Bucket ID\n        blob_prefix (str): The blob prefix pattern that wil be downloaded. Example: 'gcs-directory/tickets-20220101-'\n        destination_dir_path (str): Local destination directory path. Example: '/my-directory'\n        printout (boolean): File name will be displayed if this value is true. Default: True\n    \"\"\"\n\n    delimiter='/'\n    storage_client = storage.Client(project_id)\n    bucket=storage_client.get_bucket(bucket_id)\n    # List blobs iterate in folder \n    blobs=bucket.list_blobs(prefix=blob_prefix, delimiter=delimiter) # Excluding folder inside bucket\n    for blob in blobs:\n        if printout == True:\n            print(blob.name)\n        filename_raw = blob.name.split('/')\n        filename = filename_raw[len(filename_raw)-1]\n        destination_uri = '{}{}'.format(destination_dir_path, filename) \n        blob.download_to_filename(destination_uri)\n    results = 'OK'   \n    return results\n</code></pre>"},{"location":"gcp/#honeydew.gcp.GcpConnector.gcs_download_single_file","title":"<code>gcs_download_single_file(project_id, bucket_id, source_blob_path, destination_path)</code>","text":"<p>Download a single object from Google Cloud Storage (GCS) Args:     project_id (str): Project ID     bucket_id (str): Bucket ID     source_blob_path (str): The path of source object. Example: 'gcs-directory/my-filename.txt'     destination_path (str): Local destination path. Example: '/my-directory/my-filename.txt'</p> <p>Returns:</p> Name Type Description <code>result</code> <code>result</code> <p>Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result</p> Source code in <code>honeydew/gcp.py</code> Python<pre><code>def gcs_download_single_file(self, project_id, bucket_id, source_blob_path, destination_path):\n    \"\"\"\n    Download a single object from Google Cloud Storage (GCS)\n    Args:\n        project_id (str): Project ID\n        bucket_id (str): Bucket ID\n        source_blob_path (str): The path of source object. Example: 'gcs-directory/my-filename.txt'\n        destination_path (str): Local destination path. Example: '/my-directory/my-filename.txt'\n\n    Returns:\n        result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result\n    \"\"\"\n    gcs_client = storage.Client(project=project_id)\n    bucket = gcs_client.bucket(bucket_id)\n    blob = bucket.blob(source_blob_path)\n    blob.download_to_filename(destination_path)\n    results = 'OK'\n    return results\n</code></pre>"},{"location":"gcp/#honeydew.gcp.GcpConnector.gcs_upload_single_file","title":"<code>gcs_upload_single_file(project_id, bucket_id, local_file, destination_blob)</code>","text":"<p>Upload a single object from Google Cloud Storage (GCS) Args:     project_id (str): Project ID     bucket_id (str): Bucket ID     local_file (str): Local file as source. Example: '/local-directory/my-filename.txt'     destination_blob (str): Destination blob in GCS bucket. Example: 'gcs-directory/my-filename.txt'</p> <p>Returns:</p> Name Type Description <code>result</code> <code>str</code> <p>It returns 'OK' when successful</p> Source code in <code>honeydew/gcp.py</code> Python<pre><code>def gcs_upload_single_file(self, project_id, bucket_id, local_file, destination_blob):\n    \"\"\"\n    Upload a single object from Google Cloud Storage (GCS)\n    Args:\n        project_id (str): Project ID\n        bucket_id (str): Bucket ID\n        local_file (str): Local file as source. Example: '/local-directory/my-filename.txt'\n        destination_blob (str): Destination blob in GCS bucket. Example: 'gcs-directory/my-filename.txt'\n\n    Returns:\n        result (str): It returns 'OK' when successful\n    \"\"\"\n    gcs_client = storage.Client(project=project_id)\n    bucket = gcs_client.bucket(bucket_id)\n    blob = bucket.blob(destination_blob)\n    blob.upload_from_filename(local_file)\n    results = 'OK'\n    return results            \n</code></pre>"},{"location":"how-to-guides/","title":"MySQL","text":""},{"location":"how-to-guides/#importing-a-csv-file-into-mysql-by-replacing-a-table","title":"Importing a CSV file into MySQL by replacing a table","text":"load_csv_truncate.py<pre><code>from honeydew import MysqlConnector\n\n# Instantiate a mysql connector\nmysql_conn = MysqlConnector(host = 'mysql.mydomain.com', port = '3306', user = 'my_user', password = 'my_password', allow_local_infile=True)\n\n# Import a CSV file into a table by replacing the content\nresult = mysql_conn.load_csv_local(\n    db_name='db_name',\n    table_name='table_name',\n    file_name='test.csv',\n    write_disposition='WRITE_TRUNCATE'\n)\n</code></pre>"},{"location":"how-to-guides/#importing-a-csv-file-into-mysql-by-appending-a-table","title":"Importing a CSV file into MySQL by appending a table","text":"load_csv_append.py<pre><code>from honeydew import MysqlConnector\n\n# Instantiate a mysql connector\nmysql_conn = MysqlConnector(db_type = 'mysql', host = 'mysql.mydomain.com', port = '3306', user = 'my_user', password = 'my_password', allow_local_infile=True)\n\n# Import a CSV file into a table by replacing the content\nresult = mysql_conn.load_csv_local(\n    db_name='db_name',\n    table_name='table_name',\n    file_name='test.csv',\n    write_disposition='WRITE_APPEND'\n)\n</code></pre>"},{"location":"how-to-guides/#gcp","title":"GCP","text":""},{"location":"how-to-guides/#instantiating-a-gcp-connector-with-or-without-proxy","title":"Instantiating a GCP connector with or without proxy","text":"init_gcp.py<pre><code>from honeydew import GcpConnector\n\n# Instantiate a GCP connector with internet connection behind proxy\ng_client = GcpConnector(credential_file='my-secret-credential.json', proxy='http://proxy.mydomain.com:8080')\n\n# Instantiate a GCP connector with direct internet connection\ng_client = GcpConnector(credential_file='my-secret-credential.json')\n</code></pre>"},{"location":"how-to-guides/#querying-bigquery-table-and-store-the-result-into-a-dataframe","title":"Querying BigQuery table and store the result into a DataFrame","text":"query_to_df.py<pre><code>from honeydew import GcpConnector\n\n# Instantiate a GCP connector with internet connection behind proxy\ng_client = GcpConnector(credential_file='my-secret-credential.json', proxy='http://proxy.mydomain.com:8080')\nproject_id = 'sweet-honeydew-125283'\n\n# Submit a query and store the result into a DataFrame\nquery = 'SELECT * FROM `sweet-honeydew-125283.universe.galaxies` LIMIT 5'\ndf = g_client.bq_query_to_dataframe(project_id, query)\nprint(df.head())\n</code></pre>"},{"location":"mysql/","title":"MySQL Connector","text":""},{"location":"mysql/#honeydew.mysql.MysqlConnector","title":"<code>MysqlConnector</code>","text":"<p>Instantiate a DB connector. Args:     host (str): Database host      port (str): Database port     user (str): Username     password (str): Password     allow_local_infile (boolean): Local infile is allowed when the value is True Returns:     result (str): Value is 'OK' when successful</p> Source code in <code>honeydew/mysql.py</code> Python<pre><code>class MysqlConnector:\n    \"\"\"Instantiate a DB connector.\n    Args:\n        host (str): Database host \n        port (str): Database port\n        user (str): Username\n        password (str): Password\n        allow_local_infile (boolean): Local infile is allowed when the value is True\n    Returns:\n        result (str): Value is 'OK' when successful\n    \"\"\"   \n\n    def __init__(self, host, port, user, password, allow_local_infile=False):\n        self.host = host\n        self.port = port\n        self.user = user\n        self.password = password\n        self.allow_local_infile = allow_local_infile\n\n        self.db_connection = sql.connect(\n            host=self.host,\n            port=self.port,\n            user=self.user,\n            password=self.password,\n            ssl_disabled=True,\n            autocommit=True,\n            allow_local_infile=True\n        )    \n        self.db_cursor = self.db_connection.cursor()\n        if allow_local_infile:\n            query_str = \"\"\"SET GLOBAL local_infile=1\"\"\"\n            self.db_cursor.execute(query_str)\n\n    def query_without_fetch(self, query_str):\n        \"\"\"\n        Send non DQL query\n        Args:\n            query_str (str): sql query\n        Returns:\n            result (str): Value is 'OK' when successful\n        \"\"\"\n        self.db_cursor.execute(query_str)\n        return 'OK'\n\n    def query_to_dataframe(self, query_str):\n        \"\"\"\n        Query and store the result in a dataframe\n        Args:\n            query_str (str): sql query\n        Returns:\n            result (dataframe): Result in a dataframe\n        \"\"\"\n        self.db_cursor.execute(query_str)\n        table_rows = self.db_cursor.fetchall()\n        df = pd.DataFrame(table_rows, columns=self.db_cursor.column_names)\n        return df\n\n    def load_csv(\n        self,\n        db_name, \n        table_name, \n        file_name,\n        write_disposition,\n        delimiter=',', \n        ignore_rows=1,\n        is_local_csv=True\n    ):\n        \"\"\"\n        Load a local CSV file into a table\n        Args:\n            db_name (str): Database name where the CSV will be loaded\n            table_name (str): Table name where the CSV will be loaded\n            file_name (str): CSV file name\n            delimiter (str): CSV delimiter character\n            ignore_rows (str): Number of rows that will be ignored from the top\n            write_disposition (str): Write method to add data into table (WRITE_TRUNCATE, WRITE_APPEND)\n            is_local_csv (boolean): If the value is True, then CSV file is in local machine. If the value is False, then CSV file is in remote machine.\n        Returns:\n            result (str): The result of function\n        \"\"\"\n        result = ''\n\n        if write_disposition == 'WRITE_TRUNCATE':\n            query = 'TRUNCATE TABLE {db_name}.{table_name}'.format(db_name=db_name, table_name=table_name)\n            self.db_cursor.execute(query)\n            self.db_connection.commit()\n\n        # load table\n        if is_local_csv:\n            sql_import_table = (\"\"\" LOAD DATA LOCAL INFILE '{file_name}' \n                                    INTO TABLE {db_name}.{table_name}\n                                    FIELDS TERMINATED BY '{delimiter}' \n                                    LINES TERMINATED BY '\\\\n'\n                                    IGNORE {ignore_rows} ROWS;\n            \"\"\").format(file_name=file_name, db_name=db_name, table_name=table_name, delimiter=delimiter, ignore_rows=ignore_rows)\n        else:\n            sql_import_table = (\"\"\" LOAD DATA INFILE '{file_name}' \n                                    INTO TABLE {db_name}.{table_name}\n                                    FIELDS TERMINATED BY '{delimiter}' \n                                    LINES TERMINATED BY '\\\\n'\n                                    IGNORE {ignore_rows} ROWS;\n            \"\"\").format(file_name=file_name, db_name=db_name, table_name=table_name, delimiter=delimiter, ignore_rows=ignore_rows)\n\n        self.db_cursor.execute(sql_import_table)\n        self.db_connection.commit()\n        result = 'OK'\n        return result\n</code></pre>"},{"location":"mysql/#honeydew.mysql.MysqlConnector.load_csv","title":"<code>load_csv(db_name, table_name, file_name, write_disposition, delimiter=',', ignore_rows=1, is_local_csv=True)</code>","text":"<p>Load a local CSV file into a table Args:     db_name (str): Database name where the CSV will be loaded     table_name (str): Table name where the CSV will be loaded     file_name (str): CSV file name     delimiter (str): CSV delimiter character     ignore_rows (str): Number of rows that will be ignored from the top     write_disposition (str): Write method to add data into table (WRITE_TRUNCATE, WRITE_APPEND)     is_local_csv (boolean): If the value is True, then CSV file is in local machine. If the value is False, then CSV file is in remote machine. Returns:     result (str): The result of function</p> Source code in <code>honeydew/mysql.py</code> Python<pre><code>def load_csv(\n    self,\n    db_name, \n    table_name, \n    file_name,\n    write_disposition,\n    delimiter=',', \n    ignore_rows=1,\n    is_local_csv=True\n):\n    \"\"\"\n    Load a local CSV file into a table\n    Args:\n        db_name (str): Database name where the CSV will be loaded\n        table_name (str): Table name where the CSV will be loaded\n        file_name (str): CSV file name\n        delimiter (str): CSV delimiter character\n        ignore_rows (str): Number of rows that will be ignored from the top\n        write_disposition (str): Write method to add data into table (WRITE_TRUNCATE, WRITE_APPEND)\n        is_local_csv (boolean): If the value is True, then CSV file is in local machine. If the value is False, then CSV file is in remote machine.\n    Returns:\n        result (str): The result of function\n    \"\"\"\n    result = ''\n\n    if write_disposition == 'WRITE_TRUNCATE':\n        query = 'TRUNCATE TABLE {db_name}.{table_name}'.format(db_name=db_name, table_name=table_name)\n        self.db_cursor.execute(query)\n        self.db_connection.commit()\n\n    # load table\n    if is_local_csv:\n        sql_import_table = (\"\"\" LOAD DATA LOCAL INFILE '{file_name}' \n                                INTO TABLE {db_name}.{table_name}\n                                FIELDS TERMINATED BY '{delimiter}' \n                                LINES TERMINATED BY '\\\\n'\n                                IGNORE {ignore_rows} ROWS;\n        \"\"\").format(file_name=file_name, db_name=db_name, table_name=table_name, delimiter=delimiter, ignore_rows=ignore_rows)\n    else:\n        sql_import_table = (\"\"\" LOAD DATA INFILE '{file_name}' \n                                INTO TABLE {db_name}.{table_name}\n                                FIELDS TERMINATED BY '{delimiter}' \n                                LINES TERMINATED BY '\\\\n'\n                                IGNORE {ignore_rows} ROWS;\n        \"\"\").format(file_name=file_name, db_name=db_name, table_name=table_name, delimiter=delimiter, ignore_rows=ignore_rows)\n\n    self.db_cursor.execute(sql_import_table)\n    self.db_connection.commit()\n    result = 'OK'\n    return result\n</code></pre>"},{"location":"mysql/#honeydew.mysql.MysqlConnector.query_to_dataframe","title":"<code>query_to_dataframe(query_str)</code>","text":"<p>Query and store the result in a dataframe Args:     query_str (str): sql query Returns:     result (dataframe): Result in a dataframe</p> Source code in <code>honeydew/mysql.py</code> Python<pre><code>def query_to_dataframe(self, query_str):\n    \"\"\"\n    Query and store the result in a dataframe\n    Args:\n        query_str (str): sql query\n    Returns:\n        result (dataframe): Result in a dataframe\n    \"\"\"\n    self.db_cursor.execute(query_str)\n    table_rows = self.db_cursor.fetchall()\n    df = pd.DataFrame(table_rows, columns=self.db_cursor.column_names)\n    return df\n</code></pre>"},{"location":"mysql/#honeydew.mysql.MysqlConnector.query_without_fetch","title":"<code>query_without_fetch(query_str)</code>","text":"<p>Send non DQL query Args:     query_str (str): sql query Returns:     result (str): Value is 'OK' when successful</p> Source code in <code>honeydew/mysql.py</code> Python<pre><code>def query_without_fetch(self, query_str):\n    \"\"\"\n    Send non DQL query\n    Args:\n        query_str (str): sql query\n    Returns:\n        result (str): Value is 'OK' when successful\n    \"\"\"\n    self.db_cursor.execute(query_str)\n    return 'OK'\n</code></pre>"},{"location":"ssh/","title":"SSH Connector","text":""},{"location":"ssh/#honeydew.ssh.SshConnector","title":"<code>SshConnector</code>","text":"<p>Instantiate an SSH connector. Args:     host (str): SSH host     port (str): SSH port     private_key (str): SSH private key file path     username (str): SSH user     disable_rsa_512_256 (boolean): If the value is True, then rsa-sha2-512 and rsa-sha2-256 algorithm will be disabled</p> Source code in <code>honeydew/ssh.py</code> Python<pre><code>class SshConnector:\n    \"\"\"Instantiate an SSH connector.\n    Args:\n        host (str): SSH host\n        port (str): SSH port\n        private_key (str): SSH private key file path\n        username (str): SSH user\n        disable_rsa_512_256 (boolean): If the value is True, then rsa-sha2-512 and rsa-sha2-256 algorithm will be disabled\n    \"\"\"    \n    def __init__(self, host, port, private_key, username, disable_rsa_512_256=False):\n        self.host = host\n        self.port = port\n        self.private_key = private_key\n        self.username = username\n        self.disable_rsa_512_256 = disable_rsa_512_256\n\n        self.ssh = SSHClient()\n        self.ssh.load_system_host_keys()\n        self.ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n        self.key = paramiko.RSAKey.from_private_key_file(self.private_key)\n        if self.disable_rsa_512_256 == False:\n            self.ssh.connect(self.host, port=self.port, username=self.username, pkey=self.key, timeout=3600)\n        else:\n            self.ssh.connect(self.host, port=self.port, username=self.username, pkey=self.key, timeout=3600, disabled_algorithms=dict(pubkeys=[\"rsa-sha2-512\", \"rsa-sha2-256\"]))\n\n    def scp_upload(self, src, dst):\n        \"\"\"\n        Upload a file with SCP\n        Args:\n            src (str): Path of source file\n            dst (str): Path of destination file\n        Returns:\n            result (str): The result of function\n        \"\"\"        \n        scp = SCPClient(self.ssh.get_transport())\n        scp.put(src, dst)    \n        return \"\"\"{src} has been uploaded!\"\"\".format(src=src)\n\n    def scp_download(self, src, dst):\n        \"\"\"\n        Download a file with SCP\n        Args:\n            src (str): Path of source file\n            dst (str): Path of destination file\n        Returns:\n            result (str): The result of function\n        \"\"\"                \n        scp = SCPClient(self.ssh.get_transport())\n        scp.get(src, dst)    \n        return \"\"\"{src} has been downloaded!\"\"\".format(src=src)\n</code></pre>"},{"location":"ssh/#honeydew.ssh.SshConnector.scp_download","title":"<code>scp_download(src, dst)</code>","text":"<p>Download a file with SCP Args:     src (str): Path of source file     dst (str): Path of destination file Returns:     result (str): The result of function</p> Source code in <code>honeydew/ssh.py</code> Python<pre><code>def scp_download(self, src, dst):\n    \"\"\"\n    Download a file with SCP\n    Args:\n        src (str): Path of source file\n        dst (str): Path of destination file\n    Returns:\n        result (str): The result of function\n    \"\"\"                \n    scp = SCPClient(self.ssh.get_transport())\n    scp.get(src, dst)    \n    return \"\"\"{src} has been downloaded!\"\"\".format(src=src)\n</code></pre>"},{"location":"ssh/#honeydew.ssh.SshConnector.scp_upload","title":"<code>scp_upload(src, dst)</code>","text":"<p>Upload a file with SCP Args:     src (str): Path of source file     dst (str): Path of destination file Returns:     result (str): The result of function</p> Source code in <code>honeydew/ssh.py</code> Python<pre><code>def scp_upload(self, src, dst):\n    \"\"\"\n    Upload a file with SCP\n    Args:\n        src (str): Path of source file\n        dst (str): Path of destination file\n    Returns:\n        result (str): The result of function\n    \"\"\"        \n    scp = SCPClient(self.ssh.get_transport())\n    scp.put(src, dst)    \n    return \"\"\"{src} has been uploaded!\"\"\".format(src=src)\n</code></pre>"},{"location":"utils/","title":"Utility Functions","text":""},{"location":"utils/#honeydew.utils.Utils","title":"<code>Utils</code>","text":"<p>Instantiate utilities.</p> Source code in <code>honeydew/utils.py</code> Python<pre><code>class Utils:\n    \"\"\"Instantiate utilities.\n    \"\"\"    \n    def __init__(self, name=''):\n        \"\"\"Instantiate utilities.\n        \"\"\"    \n\n    def convert_dt_to_epoch(self, dt):\n        \"\"\"\n        Convert datetime in UTC time zone to epoch (unix time)\n        Args:\n            dt (datetime): datetime\n        Returns:\n            result (int): epoch or unix time\n        \"\"\"\n        return calendar.timegm(dt.utctimetuple())\n\n    def convert_epoch_to_dt(self, epoch):\n        \"\"\"\n        Convert epoch to datetime\n        Args:\n            epoch (int): epoch or unix time\n        Returns:\n            result (datetime): datetime\n        \"\"\"\n        if epoch &gt; 9999999999:\n            epoch = round(epoch/1000)\n        return datetime.fromtimestamp(s)\n</code></pre>"},{"location":"utils/#honeydew.utils.Utils.__init__","title":"<code>__init__(name='')</code>","text":"<p>Instantiate utilities.</p> Source code in <code>honeydew/utils.py</code> Python<pre><code>def __init__(self, name=''):\n    \"\"\"Instantiate utilities.\n    \"\"\"    \n</code></pre>"},{"location":"utils/#honeydew.utils.Utils.convert_dt_to_epoch","title":"<code>convert_dt_to_epoch(dt)</code>","text":"<p>Convert datetime in UTC time zone to epoch (unix time) Args:     dt (datetime): datetime Returns:     result (int): epoch or unix time</p> Source code in <code>honeydew/utils.py</code> Python<pre><code>def convert_dt_to_epoch(self, dt):\n    \"\"\"\n    Convert datetime in UTC time zone to epoch (unix time)\n    Args:\n        dt (datetime): datetime\n    Returns:\n        result (int): epoch or unix time\n    \"\"\"\n    return calendar.timegm(dt.utctimetuple())\n</code></pre>"},{"location":"utils/#honeydew.utils.Utils.convert_epoch_to_dt","title":"<code>convert_epoch_to_dt(epoch)</code>","text":"<p>Convert epoch to datetime Args:     epoch (int): epoch or unix time Returns:     result (datetime): datetime</p> Source code in <code>honeydew/utils.py</code> Python<pre><code>def convert_epoch_to_dt(self, epoch):\n    \"\"\"\n    Convert epoch to datetime\n    Args:\n        epoch (int): epoch or unix time\n    Returns:\n        result (datetime): datetime\n    \"\"\"\n    if epoch &gt; 9999999999:\n        epoch = round(epoch/1000)\n    return datetime.fromtimestamp(s)\n</code></pre>"},{"location":"utils/#honeydew.utils.compress","title":"<code>compress(dir_path, file_pattern='*', delete_after_compression=False)</code>","text":"<p>Compress files with pattern or prefix from a directory. Type \"*\" for all files. Args:     dir_path (str): Directory path     file_pattern (str): File pattern or prefix     delete_after_zip (bool): Delete original files after compression</p> Source code in <code>honeydew/utils.py</code> Python<pre><code>def compress(dir_path, file_pattern='*', delete_after_compression=False):\n    \"\"\"Compress files with pattern or prefix from a directory. Type \"*\" for all files.\n    Args:\n        dir_path (str): Directory path\n        file_pattern (str): File pattern or prefix\n        delete_after_zip (bool): Delete original files after compression\n    \"\"\"\n\n    import gzip\n    import shutil\n    from pathlib import Path\n    for p in Path(dir_path).glob(file_pattern):\n        with open(p, 'rb') as f_in:\n            with gzip.open(f\"{p}.gz\", 'wb') as f_out:\n                shutil.copyfileobj(f_in, f_out)\n        if delete_after_compression:\n            os.remove(p)\n</code></pre>"},{"location":"utils/#honeydew.utils.compress_parallel","title":"<code>compress_parallel(dir_path, file_pattern='*', delete_after_compressing=False)</code>","text":"<p>Compress files with pattern or prefix from a directory parallely. Type \"*\" for all files.</p> Source code in <code>honeydew/utils.py</code> Python<pre><code>def compress_parallel(dir_path, file_pattern='*', delete_after_compressing=False):\n    \"\"\"Compress files with pattern or prefix from a directory parallely. Type \"*\" for all files.\"\"\"\n    from pathlib import Path  \n    from pigz_python import PigzFile\n    import os\n    for p in Path(dir_path).glob(file_pattern):\n        pigz_file = PigzFile(p)\n        pigz_file.process_compression_target()\n        if delete_after_compressing:\n            os.remove(p)            \n</code></pre>"}]}