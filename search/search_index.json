{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Honeydew Documentation Honeydew library is a collection of ETL functions. Feel free to use and have fun with your next ETL workflows! Supported connectors: MySQL GCP SSH Utility functions: Utils Installation To install honeydew, run the following command from the command line: pip install honeydew --upgrade Repository Check out the source code here ! Cheers! Poltak","title":"Honeydew Docs"},{"location":"#welcome-to-honeydew-documentation","text":"Honeydew library is a collection of ETL functions. Feel free to use and have fun with your next ETL workflows!","title":"Welcome to Honeydew Documentation"},{"location":"#supported-connectors","text":"MySQL GCP SSH","title":"Supported connectors:"},{"location":"#utility-functions","text":"Utils","title":"Utility functions:"},{"location":"#installation","text":"To install honeydew, run the following command from the command line: pip install honeydew --upgrade","title":"Installation"},{"location":"#repository","text":"Check out the source code here ! Cheers! Poltak","title":"Repository"},{"location":"gcp/","text":"GCP Connector GcpConnector Instantiate a GCP connector. Parameters: Name Type Description Default credential_file str Credential json file required proxy str Proxy address '' Source code in honeydew/gcp.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 class GcpConnector : \"\"\" Instantiate a GCP connector. Args: credential_file (str): Credential json file proxy (str): Proxy address \"\"\" def __init__ ( self , credential_file , proxy = '' ): self . credential_file = credential_file self . proxy = proxy os . environ [ \"GOOGLE_APPLICATION_CREDENTIALS\" ] = credential_file if proxy != '' : os . environ [ 'HTTP_PROXY' ] = proxy os . environ [ 'HTTPS_PROXY' ] = proxy def bq_query_to_dataframe ( self , project_id , query , timeout = 3600 , method = 1 ): \"\"\" Submit query to BigQuery and store result into pandas dataframe Args: project_id (str): Project ID query (str): SQL query timeout (int): Query timeout in seconds method (int): API that will be used to query (1: google-cloud-bigquery, 2: pandas-gbq) Returns: result (dataframe)): Result in pandas dataframe \"\"\" df = pd . DataFrame () bqclient = bigquery . Client ( project = project_id ) query_job = bqclient . query ( query ) if method == 2 : df = pd . read_gbq ( query = query , project_id = project_id ) else : rows = list ( query_job . result ( timeout = timeout )) df = pd . DataFrame ( data = [ list ( x . values ()) for x in rows ], columns = list ( rows [ 0 ] . keys ())) return df def bq_query_non_dql ( self , project_id , query ): \"\"\" Submit non Data Query Language (DQL) type of query to BigQuery. Example: CREATE, DROP, TRUNCATE, INSERT, UPDATE, DELETE Args: project_id (str): Project ID query (str): SQL query Returns: result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result \"\"\" bqclient = bigquery . Client ( project = project_id ) query_job = bqclient . query ( query ) results = query_job . result () return results def bq_export_table_to_gcs ( self , project_id , dataset_id , table_id , gcs_uri , format = 'CSV' , delimiter = ',' , enable_compression = True , compression = 'GZIP' , overwrite = True , region = 'northamerica-northeast1' ): \"\"\" Export BigQuery table into Google Cloud Storage (GCS) Args: project_id (str): Project ID table_id (str): Table ID dataset_id (str): Dataset ID gcs_uri (str): GCS URI as destination. Example: 'gs://my-bucket/my-dir/tickets-20220101-*.csv.gz' format (str): File format (CSV, JSON, Avro, Parquet). Default: 'CSV' delimiter (str): CSV delimiter character. Default: ',' enable_compression (boolean): Files will be compressed if the value is True. Default: True compression (str): Compression format. Default: GZIP. Reference: https://cloud.google.com/bigquery/docs/exporting-data#export_formats_and_compression_types overwrite (boolean): GCS URI destination will be overwritten if the value is True. Default: True region (str): Region to run the process. Default: 'northamerica-northeast1' Returns: result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result \"\"\" bqclient = bigquery . Client ( project = project_id ) dataset_ref = bigquery . DatasetReference ( project_id , dataset_id ) table_ref = dataset_ref . table ( table_id ) job_config = bigquery . job . ExtractJobConfig () if enable_compression == True : if compression == 'DEFLATE' : job_config . compression = bigquery . Compression . DEFLATE if compression == 'SNAPPY' : job_config . compression = bigquery . Compression . SNAPPY else : job_config . compression = bigquery . Compression . GZIP extract_job = bqclient . extract_table ( table_ref , gcs_uri , location = region , job_config = job_config ) results = extract_job . result () return results def gcs_download_single_file ( self , project_id , bucket_id , source_blob_path , destination_path ): \"\"\" Download a single object from Google Cloud Storage (GCS) Args: project_id (str): Project ID bucket_id (str): Bucket ID source_blob_path (str): The path of source object. Example: 'gcs-directory/my-filename.txt' destination_path (str): Local destination path. Example: '/my-directory/my-filename.txt' Returns: result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result \"\"\" gcs_client = storage . Client ( project = project_id ) bucket = gcs_client . bucket ( bucket_id ) blob = bucket . blob ( source_blob_path ) blob . download_to_filename ( destination_path ) results = 'OK' return results def gcs_download_objects_with_pattern ( self , project_id , bucket_id , blob_prefix , destination_dir_path , printout = True ): \"\"\" Download multiple objects which have same prefix pattern from Google Cloud Storage (GCS) Args: project_id (str): Project ID bucket_id (str): Bucket ID blob_prefix (str): The blob prefix pattern that wil be downloaded. Example: 'gcs-directory/tickets-20220101-' destination_dir_path (str): Local destination directory path. Example: '/my-directory' printout (boolean): File name will be displayed if this value is true. Default: True \"\"\" delimiter = '/' storage_client = storage . Client ( project_id ) bucket = storage_client . get_bucket ( bucket_id ) # List blobs iterate in folder blobs = bucket . list_blobs ( prefix = blob_prefix , delimiter = delimiter ) # Excluding folder inside bucket for blob in blobs : if printout == True : print ( blob . name ) filename_raw = blob . name . split ( '/' ) filename = filename_raw [ len ( filename_raw ) - 1 ] destination_uri = ' {}{} ' . format ( destination_dir_path , filename ) blob . download_to_filename ( destination_uri ) results = 'OK' return results def gcs_upload_single_file ( self , project_id , bucket_id , local_file , destination_blob ): \"\"\" Upload a single object from Google Cloud Storage (GCS) Args: project_id (str): Project ID bucket_id (str): Bucket ID local_file (str): Local file as source. Example: '/local-directory/my-filename.txt' destination_blob (str): Destination blob in GCS bucket. Example: 'gcs-directory/my-filename.txt' Returns: result (str): It returns 'OK' when successful \"\"\" gcs_client = storage . Client ( project = project_id ) bucket = gcs_client . bucket ( bucket_id ) blob = bucket . blob ( destination_blob ) blob . upload_from_filename ( local_file ) results = 'OK' return results bq_export_table_to_gcs ( project_id , dataset_id , table_id , gcs_uri , format = 'CSV' , delimiter = ',' , enable_compression = True , compression = 'GZIP' , overwrite = True , region = 'northamerica-northeast1' ) Export BigQuery table into Google Cloud Storage (GCS) Parameters: Name Type Description Default project_id str Project ID required table_id str Table ID required dataset_id str Dataset ID required gcs_uri str GCS URI as destination. Example: 'gs://my-bucket/my-dir/tickets-20220101-*.csv.gz' required format str File format (CSV, JSON, Avro, Parquet). Default: 'CSV' 'CSV' delimiter str CSV delimiter character. Default: ',' ',' enable_compression boolean Files will be compressed if the value is True. Default: True True compression str Compression format. Default: GZIP. Reference: https://cloud.google.com/bigquery/docs/exporting-data#export_formats_and_compression_types 'GZIP' overwrite boolean GCS URI destination will be overwritten if the value is True. Default: True True region str Region to run the process. Default: 'northamerica-northeast1' 'northamerica-northeast1' Returns: Name Type Description result result Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result Source code in honeydew/gcp.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def bq_export_table_to_gcs ( self , project_id , dataset_id , table_id , gcs_uri , format = 'CSV' , delimiter = ',' , enable_compression = True , compression = 'GZIP' , overwrite = True , region = 'northamerica-northeast1' ): \"\"\" Export BigQuery table into Google Cloud Storage (GCS) Args: project_id (str): Project ID table_id (str): Table ID dataset_id (str): Dataset ID gcs_uri (str): GCS URI as destination. Example: 'gs://my-bucket/my-dir/tickets-20220101-*.csv.gz' format (str): File format (CSV, JSON, Avro, Parquet). Default: 'CSV' delimiter (str): CSV delimiter character. Default: ',' enable_compression (boolean): Files will be compressed if the value is True. Default: True compression (str): Compression format. Default: GZIP. Reference: https://cloud.google.com/bigquery/docs/exporting-data#export_formats_and_compression_types overwrite (boolean): GCS URI destination will be overwritten if the value is True. Default: True region (str): Region to run the process. Default: 'northamerica-northeast1' Returns: result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result \"\"\" bqclient = bigquery . Client ( project = project_id ) dataset_ref = bigquery . DatasetReference ( project_id , dataset_id ) table_ref = dataset_ref . table ( table_id ) job_config = bigquery . job . ExtractJobConfig () if enable_compression == True : if compression == 'DEFLATE' : job_config . compression = bigquery . Compression . DEFLATE if compression == 'SNAPPY' : job_config . compression = bigquery . Compression . SNAPPY else : job_config . compression = bigquery . Compression . GZIP extract_job = bqclient . extract_table ( table_ref , gcs_uri , location = region , job_config = job_config ) results = extract_job . result () return results bq_query_non_dql ( project_id , query ) Submit non Data Query Language (DQL) type of query to BigQuery. Example: CREATE, DROP, TRUNCATE, INSERT, UPDATE, DELETE Parameters: Name Type Description Default project_id str Project ID required query str SQL query required Returns: Name Type Description result result Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result Source code in honeydew/gcp.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def bq_query_non_dql ( self , project_id , query ): \"\"\" Submit non Data Query Language (DQL) type of query to BigQuery. Example: CREATE, DROP, TRUNCATE, INSERT, UPDATE, DELETE Args: project_id (str): Project ID query (str): SQL query Returns: result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result \"\"\" bqclient = bigquery . Client ( project = project_id ) query_job = bqclient . query ( query ) results = query_job . result () return results bq_query_to_dataframe ( project_id , query , timeout = 3600 , method = 1 ) Submit query to BigQuery and store result into pandas dataframe Parameters: Name Type Description Default project_id str Project ID required query str SQL query required timeout int Query timeout in seconds 3600 method int API that will be used to query (1: google-cloud-bigquery, 2: pandas-gbq) 1 Returns: Name Type Description result dataframe) Result in pandas dataframe Source code in honeydew/gcp.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def bq_query_to_dataframe ( self , project_id , query , timeout = 3600 , method = 1 ): \"\"\" Submit query to BigQuery and store result into pandas dataframe Args: project_id (str): Project ID query (str): SQL query timeout (int): Query timeout in seconds method (int): API that will be used to query (1: google-cloud-bigquery, 2: pandas-gbq) Returns: result (dataframe)): Result in pandas dataframe \"\"\" df = pd . DataFrame () bqclient = bigquery . Client ( project = project_id ) query_job = bqclient . query ( query ) if method == 2 : df = pd . read_gbq ( query = query , project_id = project_id ) else : rows = list ( query_job . result ( timeout = timeout )) df = pd . DataFrame ( data = [ list ( x . values ()) for x in rows ], columns = list ( rows [ 0 ] . keys ())) return df gcs_download_objects_with_pattern ( project_id , bucket_id , blob_prefix , destination_dir_path , printout = True ) Download multiple objects which have same prefix pattern from Google Cloud Storage (GCS) Parameters: Name Type Description Default project_id str Project ID required bucket_id str Bucket ID required blob_prefix str The blob prefix pattern that wil be downloaded. Example: 'gcs-directory/tickets-20220101-' required destination_dir_path str Local destination directory path. Example: '/my-directory' required printout boolean File name will be displayed if this value is true. Default: True True Source code in honeydew/gcp.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def gcs_download_objects_with_pattern ( self , project_id , bucket_id , blob_prefix , destination_dir_path , printout = True ): \"\"\" Download multiple objects which have same prefix pattern from Google Cloud Storage (GCS) Args: project_id (str): Project ID bucket_id (str): Bucket ID blob_prefix (str): The blob prefix pattern that wil be downloaded. Example: 'gcs-directory/tickets-20220101-' destination_dir_path (str): Local destination directory path. Example: '/my-directory' printout (boolean): File name will be displayed if this value is true. Default: True \"\"\" delimiter = '/' storage_client = storage . Client ( project_id ) bucket = storage_client . get_bucket ( bucket_id ) # List blobs iterate in folder blobs = bucket . list_blobs ( prefix = blob_prefix , delimiter = delimiter ) # Excluding folder inside bucket for blob in blobs : if printout == True : print ( blob . name ) filename_raw = blob . name . split ( '/' ) filename = filename_raw [ len ( filename_raw ) - 1 ] destination_uri = ' {}{} ' . format ( destination_dir_path , filename ) blob . download_to_filename ( destination_uri ) results = 'OK' return results gcs_download_single_file ( project_id , bucket_id , source_blob_path , destination_path ) Download a single object from Google Cloud Storage (GCS) Parameters: Name Type Description Default project_id str Project ID required bucket_id str Bucket ID required source_blob_path str The path of source object. Example: 'gcs-directory/my-filename.txt' required destination_path str Local destination path. Example: '/my-directory/my-filename.txt' required Returns: Name Type Description result result Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result Source code in honeydew/gcp.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def gcs_download_single_file ( self , project_id , bucket_id , source_blob_path , destination_path ): \"\"\" Download a single object from Google Cloud Storage (GCS) Args: project_id (str): Project ID bucket_id (str): Bucket ID source_blob_path (str): The path of source object. Example: 'gcs-directory/my-filename.txt' destination_path (str): Local destination path. Example: '/my-directory/my-filename.txt' Returns: result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result \"\"\" gcs_client = storage . Client ( project = project_id ) bucket = gcs_client . bucket ( bucket_id ) blob = bucket . blob ( source_blob_path ) blob . download_to_filename ( destination_path ) results = 'OK' return results gcs_upload_single_file ( project_id , bucket_id , local_file , destination_blob ) Upload a single object from Google Cloud Storage (GCS) Parameters: Name Type Description Default project_id str Project ID required bucket_id str Bucket ID required local_file str Local file as source. Example: '/local-directory/my-filename.txt' required destination_blob str Destination blob in GCS bucket. Example: 'gcs-directory/my-filename.txt' required Returns: Name Type Description result str It returns 'OK' when successful Source code in honeydew/gcp.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def gcs_upload_single_file ( self , project_id , bucket_id , local_file , destination_blob ): \"\"\" Upload a single object from Google Cloud Storage (GCS) Args: project_id (str): Project ID bucket_id (str): Bucket ID local_file (str): Local file as source. Example: '/local-directory/my-filename.txt' destination_blob (str): Destination blob in GCS bucket. Example: 'gcs-directory/my-filename.txt' Returns: result (str): It returns 'OK' when successful \"\"\" gcs_client = storage . Client ( project = project_id ) bucket = gcs_client . bucket ( bucket_id ) blob = bucket . blob ( destination_blob ) blob . upload_from_filename ( local_file ) results = 'OK' return results","title":"GCP Connector"},{"location":"gcp/#gcp-connector","text":"","title":"GCP Connector"},{"location":"gcp/#honeydew.gcp.GcpConnector","text":"Instantiate a GCP connector. Parameters: Name Type Description Default credential_file str Credential json file required proxy str Proxy address '' Source code in honeydew/gcp.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 class GcpConnector : \"\"\" Instantiate a GCP connector. Args: credential_file (str): Credential json file proxy (str): Proxy address \"\"\" def __init__ ( self , credential_file , proxy = '' ): self . credential_file = credential_file self . proxy = proxy os . environ [ \"GOOGLE_APPLICATION_CREDENTIALS\" ] = credential_file if proxy != '' : os . environ [ 'HTTP_PROXY' ] = proxy os . environ [ 'HTTPS_PROXY' ] = proxy def bq_query_to_dataframe ( self , project_id , query , timeout = 3600 , method = 1 ): \"\"\" Submit query to BigQuery and store result into pandas dataframe Args: project_id (str): Project ID query (str): SQL query timeout (int): Query timeout in seconds method (int): API that will be used to query (1: google-cloud-bigquery, 2: pandas-gbq) Returns: result (dataframe)): Result in pandas dataframe \"\"\" df = pd . DataFrame () bqclient = bigquery . Client ( project = project_id ) query_job = bqclient . query ( query ) if method == 2 : df = pd . read_gbq ( query = query , project_id = project_id ) else : rows = list ( query_job . result ( timeout = timeout )) df = pd . DataFrame ( data = [ list ( x . values ()) for x in rows ], columns = list ( rows [ 0 ] . keys ())) return df def bq_query_non_dql ( self , project_id , query ): \"\"\" Submit non Data Query Language (DQL) type of query to BigQuery. Example: CREATE, DROP, TRUNCATE, INSERT, UPDATE, DELETE Args: project_id (str): Project ID query (str): SQL query Returns: result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result \"\"\" bqclient = bigquery . Client ( project = project_id ) query_job = bqclient . query ( query ) results = query_job . result () return results def bq_export_table_to_gcs ( self , project_id , dataset_id , table_id , gcs_uri , format = 'CSV' , delimiter = ',' , enable_compression = True , compression = 'GZIP' , overwrite = True , region = 'northamerica-northeast1' ): \"\"\" Export BigQuery table into Google Cloud Storage (GCS) Args: project_id (str): Project ID table_id (str): Table ID dataset_id (str): Dataset ID gcs_uri (str): GCS URI as destination. Example: 'gs://my-bucket/my-dir/tickets-20220101-*.csv.gz' format (str): File format (CSV, JSON, Avro, Parquet). Default: 'CSV' delimiter (str): CSV delimiter character. Default: ',' enable_compression (boolean): Files will be compressed if the value is True. Default: True compression (str): Compression format. Default: GZIP. Reference: https://cloud.google.com/bigquery/docs/exporting-data#export_formats_and_compression_types overwrite (boolean): GCS URI destination will be overwritten if the value is True. Default: True region (str): Region to run the process. Default: 'northamerica-northeast1' Returns: result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result \"\"\" bqclient = bigquery . Client ( project = project_id ) dataset_ref = bigquery . DatasetReference ( project_id , dataset_id ) table_ref = dataset_ref . table ( table_id ) job_config = bigquery . job . ExtractJobConfig () if enable_compression == True : if compression == 'DEFLATE' : job_config . compression = bigquery . Compression . DEFLATE if compression == 'SNAPPY' : job_config . compression = bigquery . Compression . SNAPPY else : job_config . compression = bigquery . Compression . GZIP extract_job = bqclient . extract_table ( table_ref , gcs_uri , location = region , job_config = job_config ) results = extract_job . result () return results def gcs_download_single_file ( self , project_id , bucket_id , source_blob_path , destination_path ): \"\"\" Download a single object from Google Cloud Storage (GCS) Args: project_id (str): Project ID bucket_id (str): Bucket ID source_blob_path (str): The path of source object. Example: 'gcs-directory/my-filename.txt' destination_path (str): Local destination path. Example: '/my-directory/my-filename.txt' Returns: result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result \"\"\" gcs_client = storage . Client ( project = project_id ) bucket = gcs_client . bucket ( bucket_id ) blob = bucket . blob ( source_blob_path ) blob . download_to_filename ( destination_path ) results = 'OK' return results def gcs_download_objects_with_pattern ( self , project_id , bucket_id , blob_prefix , destination_dir_path , printout = True ): \"\"\" Download multiple objects which have same prefix pattern from Google Cloud Storage (GCS) Args: project_id (str): Project ID bucket_id (str): Bucket ID blob_prefix (str): The blob prefix pattern that wil be downloaded. Example: 'gcs-directory/tickets-20220101-' destination_dir_path (str): Local destination directory path. Example: '/my-directory' printout (boolean): File name will be displayed if this value is true. Default: True \"\"\" delimiter = '/' storage_client = storage . Client ( project_id ) bucket = storage_client . get_bucket ( bucket_id ) # List blobs iterate in folder blobs = bucket . list_blobs ( prefix = blob_prefix , delimiter = delimiter ) # Excluding folder inside bucket for blob in blobs : if printout == True : print ( blob . name ) filename_raw = blob . name . split ( '/' ) filename = filename_raw [ len ( filename_raw ) - 1 ] destination_uri = ' {}{} ' . format ( destination_dir_path , filename ) blob . download_to_filename ( destination_uri ) results = 'OK' return results def gcs_upload_single_file ( self , project_id , bucket_id , local_file , destination_blob ): \"\"\" Upload a single object from Google Cloud Storage (GCS) Args: project_id (str): Project ID bucket_id (str): Bucket ID local_file (str): Local file as source. Example: '/local-directory/my-filename.txt' destination_blob (str): Destination blob in GCS bucket. Example: 'gcs-directory/my-filename.txt' Returns: result (str): It returns 'OK' when successful \"\"\" gcs_client = storage . Client ( project = project_id ) bucket = gcs_client . bucket ( bucket_id ) blob = bucket . blob ( destination_blob ) blob . upload_from_filename ( local_file ) results = 'OK' return results","title":"GcpConnector"},{"location":"gcp/#honeydew.gcp.GcpConnector.bq_export_table_to_gcs","text":"Export BigQuery table into Google Cloud Storage (GCS) Parameters: Name Type Description Default project_id str Project ID required table_id str Table ID required dataset_id str Dataset ID required gcs_uri str GCS URI as destination. Example: 'gs://my-bucket/my-dir/tickets-20220101-*.csv.gz' required format str File format (CSV, JSON, Avro, Parquet). Default: 'CSV' 'CSV' delimiter str CSV delimiter character. Default: ',' ',' enable_compression boolean Files will be compressed if the value is True. Default: True True compression str Compression format. Default: GZIP. Reference: https://cloud.google.com/bigquery/docs/exporting-data#export_formats_and_compression_types 'GZIP' overwrite boolean GCS URI destination will be overwritten if the value is True. Default: True True region str Region to run the process. Default: 'northamerica-northeast1' 'northamerica-northeast1' Returns: Name Type Description result result Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result Source code in honeydew/gcp.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def bq_export_table_to_gcs ( self , project_id , dataset_id , table_id , gcs_uri , format = 'CSV' , delimiter = ',' , enable_compression = True , compression = 'GZIP' , overwrite = True , region = 'northamerica-northeast1' ): \"\"\" Export BigQuery table into Google Cloud Storage (GCS) Args: project_id (str): Project ID table_id (str): Table ID dataset_id (str): Dataset ID gcs_uri (str): GCS URI as destination. Example: 'gs://my-bucket/my-dir/tickets-20220101-*.csv.gz' format (str): File format (CSV, JSON, Avro, Parquet). Default: 'CSV' delimiter (str): CSV delimiter character. Default: ',' enable_compression (boolean): Files will be compressed if the value is True. Default: True compression (str): Compression format. Default: GZIP. Reference: https://cloud.google.com/bigquery/docs/exporting-data#export_formats_and_compression_types overwrite (boolean): GCS URI destination will be overwritten if the value is True. Default: True region (str): Region to run the process. Default: 'northamerica-northeast1' Returns: result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result \"\"\" bqclient = bigquery . Client ( project = project_id ) dataset_ref = bigquery . DatasetReference ( project_id , dataset_id ) table_ref = dataset_ref . table ( table_id ) job_config = bigquery . job . ExtractJobConfig () if enable_compression == True : if compression == 'DEFLATE' : job_config . compression = bigquery . Compression . DEFLATE if compression == 'SNAPPY' : job_config . compression = bigquery . Compression . SNAPPY else : job_config . compression = bigquery . Compression . GZIP extract_job = bqclient . extract_table ( table_ref , gcs_uri , location = region , job_config = job_config ) results = extract_job . result () return results","title":"bq_export_table_to_gcs()"},{"location":"gcp/#honeydew.gcp.GcpConnector.bq_query_non_dql","text":"Submit non Data Query Language (DQL) type of query to BigQuery. Example: CREATE, DROP, TRUNCATE, INSERT, UPDATE, DELETE Parameters: Name Type Description Default project_id str Project ID required query str SQL query required Returns: Name Type Description result result Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result Source code in honeydew/gcp.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def bq_query_non_dql ( self , project_id , query ): \"\"\" Submit non Data Query Language (DQL) type of query to BigQuery. Example: CREATE, DROP, TRUNCATE, INSERT, UPDATE, DELETE Args: project_id (str): Project ID query (str): SQL query Returns: result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result \"\"\" bqclient = bigquery . Client ( project = project_id ) query_job = bqclient . query ( query ) results = query_job . result () return results","title":"bq_query_non_dql()"},{"location":"gcp/#honeydew.gcp.GcpConnector.bq_query_to_dataframe","text":"Submit query to BigQuery and store result into pandas dataframe Parameters: Name Type Description Default project_id str Project ID required query str SQL query required timeout int Query timeout in seconds 3600 method int API that will be used to query (1: google-cloud-bigquery, 2: pandas-gbq) 1 Returns: Name Type Description result dataframe) Result in pandas dataframe Source code in honeydew/gcp.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def bq_query_to_dataframe ( self , project_id , query , timeout = 3600 , method = 1 ): \"\"\" Submit query to BigQuery and store result into pandas dataframe Args: project_id (str): Project ID query (str): SQL query timeout (int): Query timeout in seconds method (int): API that will be used to query (1: google-cloud-bigquery, 2: pandas-gbq) Returns: result (dataframe)): Result in pandas dataframe \"\"\" df = pd . DataFrame () bqclient = bigquery . Client ( project = project_id ) query_job = bqclient . query ( query ) if method == 2 : df = pd . read_gbq ( query = query , project_id = project_id ) else : rows = list ( query_job . result ( timeout = timeout )) df = pd . DataFrame ( data = [ list ( x . values ()) for x in rows ], columns = list ( rows [ 0 ] . keys ())) return df","title":"bq_query_to_dataframe()"},{"location":"gcp/#honeydew.gcp.GcpConnector.gcs_download_objects_with_pattern","text":"Download multiple objects which have same prefix pattern from Google Cloud Storage (GCS) Parameters: Name Type Description Default project_id str Project ID required bucket_id str Bucket ID required blob_prefix str The blob prefix pattern that wil be downloaded. Example: 'gcs-directory/tickets-20220101-' required destination_dir_path str Local destination directory path. Example: '/my-directory' required printout boolean File name will be displayed if this value is true. Default: True True Source code in honeydew/gcp.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def gcs_download_objects_with_pattern ( self , project_id , bucket_id , blob_prefix , destination_dir_path , printout = True ): \"\"\" Download multiple objects which have same prefix pattern from Google Cloud Storage (GCS) Args: project_id (str): Project ID bucket_id (str): Bucket ID blob_prefix (str): The blob prefix pattern that wil be downloaded. Example: 'gcs-directory/tickets-20220101-' destination_dir_path (str): Local destination directory path. Example: '/my-directory' printout (boolean): File name will be displayed if this value is true. Default: True \"\"\" delimiter = '/' storage_client = storage . Client ( project_id ) bucket = storage_client . get_bucket ( bucket_id ) # List blobs iterate in folder blobs = bucket . list_blobs ( prefix = blob_prefix , delimiter = delimiter ) # Excluding folder inside bucket for blob in blobs : if printout == True : print ( blob . name ) filename_raw = blob . name . split ( '/' ) filename = filename_raw [ len ( filename_raw ) - 1 ] destination_uri = ' {}{} ' . format ( destination_dir_path , filename ) blob . download_to_filename ( destination_uri ) results = 'OK' return results","title":"gcs_download_objects_with_pattern()"},{"location":"gcp/#honeydew.gcp.GcpConnector.gcs_download_single_file","text":"Download a single object from Google Cloud Storage (GCS) Parameters: Name Type Description Default project_id str Project ID required bucket_id str Bucket ID required source_blob_path str The path of source object. Example: 'gcs-directory/my-filename.txt' required destination_path str Local destination path. Example: '/my-directory/my-filename.txt' required Returns: Name Type Description result result Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result Source code in honeydew/gcp.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def gcs_download_single_file ( self , project_id , bucket_id , source_blob_path , destination_path ): \"\"\" Download a single object from Google Cloud Storage (GCS) Args: project_id (str): Project ID bucket_id (str): Bucket ID source_blob_path (str): The path of source object. Example: 'gcs-directory/my-filename.txt' destination_path (str): Local destination path. Example: '/my-directory/my-filename.txt' Returns: result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result \"\"\" gcs_client = storage . Client ( project = project_id ) bucket = gcs_client . bucket ( bucket_id ) blob = bucket . blob ( source_blob_path ) blob . download_to_filename ( destination_path ) results = 'OK' return results","title":"gcs_download_single_file()"},{"location":"gcp/#honeydew.gcp.GcpConnector.gcs_upload_single_file","text":"Upload a single object from Google Cloud Storage (GCS) Parameters: Name Type Description Default project_id str Project ID required bucket_id str Bucket ID required local_file str Local file as source. Example: '/local-directory/my-filename.txt' required destination_blob str Destination blob in GCS bucket. Example: 'gcs-directory/my-filename.txt' required Returns: Name Type Description result str It returns 'OK' when successful Source code in honeydew/gcp.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def gcs_upload_single_file ( self , project_id , bucket_id , local_file , destination_blob ): \"\"\" Upload a single object from Google Cloud Storage (GCS) Args: project_id (str): Project ID bucket_id (str): Bucket ID local_file (str): Local file as source. Example: '/local-directory/my-filename.txt' destination_blob (str): Destination blob in GCS bucket. Example: 'gcs-directory/my-filename.txt' Returns: result (str): It returns 'OK' when successful \"\"\" gcs_client = storage . Client ( project = project_id ) bucket = gcs_client . bucket ( bucket_id ) blob = bucket . blob ( destination_blob ) blob . upload_from_filename ( local_file ) results = 'OK' return results","title":"gcs_upload_single_file()"},{"location":"how-to-guides/","text":"MySQL Importing a CSV file into MySQL by replacing a table from honeydew import mysql_connector # Instantiate a mysql connector mysql_conn = MysqlConnector(host = 'mysql.mydomain.com', port = '3306', user = 'my_user', password = 'my_password', allow_local_infile=True) # Import a CSV file into a table by replacing the content result = mysql_conn.load_csv_local( db_name='db_name', table_name='table_name', file_name='test.csv', write_disposition='WRITE_TRUNCATE' ) Importing a CSV file into MySQL by appending a table from honeydew import db_connector # Instantiate a mysql connector mysql_conn = db_connector(db_type = 'mysql', host = 'mysql.mydomain.com', port = '3306', user = 'my_user', password = 'my_password', allow_local_infile=True) # Import a CSV file into a table by replacing the content result = mysql_conn.load_csv_local( db_name='db_name', table_name='table_name', file_name='test.csv', write_disposition='WRITE_APPEND' ) GCP Instantiating a GCP connector with or without proxy from honeydew import GcpConnector # Instantiate a GCP connector with internet connection behind proxy g_client = gcp_connector(credential_file='my-secret-credential.json', proxy='http://proxy.mydomain.com:8080') # Instantiate a GCP connector with direct internet connection g_client = gcp_connector(credential_file='my-secret-credential.json') Querying BigQuery table and store the result into a DataFrame from honeydew import gcp_connector # Instantiate a GCP connector with internet connection behind proxy g_client = gcp_connector(credential_file='my-secret-credential.json', proxy='http://proxy.mydomain.com:8080') project_id = 'sweet-honeydew-125283' # Submit a query and store the result into a DataFrame query = 'SELECT * FROM `sweet-honeydew-125283.universe.galaxies` LIMIT 5' df = g_client.bq_query_to_dataframe(project_id, query) print(df.head())","title":"How-To Guides"},{"location":"how-to-guides/#mysql","text":"","title":"MySQL"},{"location":"how-to-guides/#importing-a-csv-file-into-mysql-by-replacing-a-table","text":"from honeydew import mysql_connector # Instantiate a mysql connector mysql_conn = MysqlConnector(host = 'mysql.mydomain.com', port = '3306', user = 'my_user', password = 'my_password', allow_local_infile=True) # Import a CSV file into a table by replacing the content result = mysql_conn.load_csv_local( db_name='db_name', table_name='table_name', file_name='test.csv', write_disposition='WRITE_TRUNCATE' )","title":"Importing a CSV file into MySQL by replacing a table"},{"location":"how-to-guides/#importing-a-csv-file-into-mysql-by-appending-a-table","text":"from honeydew import db_connector # Instantiate a mysql connector mysql_conn = db_connector(db_type = 'mysql', host = 'mysql.mydomain.com', port = '3306', user = 'my_user', password = 'my_password', allow_local_infile=True) # Import a CSV file into a table by replacing the content result = mysql_conn.load_csv_local( db_name='db_name', table_name='table_name', file_name='test.csv', write_disposition='WRITE_APPEND' )","title":"Importing a CSV file into MySQL by appending a table"},{"location":"how-to-guides/#gcp","text":"","title":"GCP"},{"location":"how-to-guides/#instantiating-a-gcp-connector-with-or-without-proxy","text":"from honeydew import GcpConnector # Instantiate a GCP connector with internet connection behind proxy g_client = gcp_connector(credential_file='my-secret-credential.json', proxy='http://proxy.mydomain.com:8080') # Instantiate a GCP connector with direct internet connection g_client = gcp_connector(credential_file='my-secret-credential.json')","title":"Instantiating a GCP connector with or without proxy"},{"location":"how-to-guides/#querying-bigquery-table-and-store-the-result-into-a-dataframe","text":"from honeydew import gcp_connector # Instantiate a GCP connector with internet connection behind proxy g_client = gcp_connector(credential_file='my-secret-credential.json', proxy='http://proxy.mydomain.com:8080') project_id = 'sweet-honeydew-125283' # Submit a query and store the result into a DataFrame query = 'SELECT * FROM `sweet-honeydew-125283.universe.galaxies` LIMIT 5' df = g_client.bq_query_to_dataframe(project_id, query) print(df.head())","title":"Querying BigQuery table and store the result into a DataFrame"},{"location":"mysql/","text":"MySQL Connector MysqlConnector Instantiate a DB connector. Parameters: Name Type Description Default host str Database host required port str Database port required user str Username required password str Password required allow_local_infile boolean Local infile is allowed when the value is True False Returns: Name Type Description result str Value is 'OK' when successful Source code in honeydew/mysql.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 class MysqlConnector : \"\"\"Instantiate a DB connector. Args: host (str): Database host port (str): Database port user (str): Username password (str): Password allow_local_infile (boolean): Local infile is allowed when the value is True Returns: result (str): Value is 'OK' when successful \"\"\" def __init__ ( self , host , port , user , password , allow_local_infile = False ): self . host = host self . port = port self . user = user self . password = password self . allow_local_infile = allow_local_infile self . db_connection = sql . connect ( host = self . host , port = self . port , user = self . user , password = self . password , ssl_disabled = True , autocommit = True , allow_local_infile = True ) self . db_cursor = self . db_connection . cursor () if allow_local_infile : query_str = \"\"\"SET GLOBAL local_infile=1\"\"\" self . db_cursor . execute ( query_str ) def query_without_fetch ( self , query_str ): \"\"\" Send non DQL query Args: query_str (str): sql query Returns: result (str): Value is 'OK' when successful \"\"\" self . db_cursor . execute ( query_str ) return 'OK' def query_to_dataframe ( self , query_str ): \"\"\" Query and store the result in a dataframe Args: query_str (str): sql query Returns: result (dataframe): Result in a dataframe \"\"\" self . db_cursor . execute ( query_str ) table_rows = self . db_cursor . fetchall () df = pd . DataFrame ( table_rows , columns = self . db_cursor . column_names ) return df def load_csv ( self , db_name , table_name , file_name , write_disposition , delimiter = ',' , ignore_rows = 1 , is_local_csv = True ): \"\"\" Load a local CSV file into a table Args: db_name (str): Database name where the CSV will be loaded table_name (str): Table name where the CSV will be loaded file_name (str): CSV file name delimiter (str): CSV delimiter character ignore_rows (str): Number of rows that will be ignored from the top write_disposition (str): Write method to add data into table (WRITE_TRUNCATE, WRITE_APPEND) is_local_csv (boolean): If the value is True, then CSV file is in local machine. If the value is False, then CSV file is in remote machine. Returns: result (str): The result of function \"\"\" result = '' if write_disposition == 'WRITE_TRUNCATE' : query = 'TRUNCATE TABLE {db_name} . {table_name} ' . format ( db_name = db_name , table_name = table_name ) self . db_cursor . execute ( query ) self . db_connection . commit () # load table if is_local_csv : sql_import_table = ( \"\"\" LOAD DATA LOCAL INFILE ' {file_name} ' INTO TABLE {db_name} . {table_name} FIELDS TERMINATED BY ' {delimiter} ' LINES TERMINATED BY ' \\\\ n' IGNORE {ignore_rows} ROWS; \"\"\" ) . format ( file_name = file_name , db_name = db_name , table_name = table_name , delimiter = delimiter , ignore_rows = ignore_rows ) else : sql_import_table = ( \"\"\" LOAD DATA INFILE ' {file_name} ' INTO TABLE {db_name} . {table_name} FIELDS TERMINATED BY ' {delimiter} ' LINES TERMINATED BY ' \\\\ n' IGNORE {ignore_rows} ROWS; \"\"\" ) . format ( file_name = file_name , db_name = db_name , table_name = table_name , delimiter = delimiter , ignore_rows = ignore_rows ) self . db_cursor . execute ( sql_import_table ) self . db_connection . commit () result = 'OK' return result load_csv ( db_name , table_name , file_name , write_disposition , delimiter = ',' , ignore_rows = 1 , is_local_csv = True ) Load a local CSV file into a table Parameters: Name Type Description Default db_name str Database name where the CSV will be loaded required table_name str Table name where the CSV will be loaded required file_name str CSV file name required delimiter str CSV delimiter character ',' ignore_rows str Number of rows that will be ignored from the top 1 write_disposition str Write method to add data into table (WRITE_TRUNCATE, WRITE_APPEND) required is_local_csv boolean If the value is True, then CSV file is in local machine. If the value is False, then CSV file is in remote machine. True Returns: Name Type Description result str The result of function Source code in honeydew/mysql.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def load_csv ( self , db_name , table_name , file_name , write_disposition , delimiter = ',' , ignore_rows = 1 , is_local_csv = True ): \"\"\" Load a local CSV file into a table Args: db_name (str): Database name where the CSV will be loaded table_name (str): Table name where the CSV will be loaded file_name (str): CSV file name delimiter (str): CSV delimiter character ignore_rows (str): Number of rows that will be ignored from the top write_disposition (str): Write method to add data into table (WRITE_TRUNCATE, WRITE_APPEND) is_local_csv (boolean): If the value is True, then CSV file is in local machine. If the value is False, then CSV file is in remote machine. Returns: result (str): The result of function \"\"\" result = '' if write_disposition == 'WRITE_TRUNCATE' : query = 'TRUNCATE TABLE {db_name} . {table_name} ' . format ( db_name = db_name , table_name = table_name ) self . db_cursor . execute ( query ) self . db_connection . commit () # load table if is_local_csv : sql_import_table = ( \"\"\" LOAD DATA LOCAL INFILE ' {file_name} ' INTO TABLE {db_name} . {table_name} FIELDS TERMINATED BY ' {delimiter} ' LINES TERMINATED BY ' \\\\ n' IGNORE {ignore_rows} ROWS; \"\"\" ) . format ( file_name = file_name , db_name = db_name , table_name = table_name , delimiter = delimiter , ignore_rows = ignore_rows ) else : sql_import_table = ( \"\"\" LOAD DATA INFILE ' {file_name} ' INTO TABLE {db_name} . {table_name} FIELDS TERMINATED BY ' {delimiter} ' LINES TERMINATED BY ' \\\\ n' IGNORE {ignore_rows} ROWS; \"\"\" ) . format ( file_name = file_name , db_name = db_name , table_name = table_name , delimiter = delimiter , ignore_rows = ignore_rows ) self . db_cursor . execute ( sql_import_table ) self . db_connection . commit () result = 'OK' return result query_to_dataframe ( query_str ) Query and store the result in a dataframe Parameters: Name Type Description Default query_str str sql query required Returns: Name Type Description result dataframe Result in a dataframe Source code in honeydew/mysql.py 53 54 55 56 57 58 59 60 61 62 63 64 def query_to_dataframe ( self , query_str ): \"\"\" Query and store the result in a dataframe Args: query_str (str): sql query Returns: result (dataframe): Result in a dataframe \"\"\" self . db_cursor . execute ( query_str ) table_rows = self . db_cursor . fetchall () df = pd . DataFrame ( table_rows , columns = self . db_cursor . column_names ) return df query_without_fetch ( query_str ) Send non DQL query Parameters: Name Type Description Default query_str str sql query required Returns: Name Type Description result str Value is 'OK' when successful Source code in honeydew/mysql.py 42 43 44 45 46 47 48 49 50 51 def query_without_fetch ( self , query_str ): \"\"\" Send non DQL query Args: query_str (str): sql query Returns: result (str): Value is 'OK' when successful \"\"\" self . db_cursor . execute ( query_str ) return 'OK'","title":"MySQL Connector"},{"location":"mysql/#mysql-connector","text":"","title":"MySQL Connector"},{"location":"mysql/#honeydew.mysql.MysqlConnector","text":"Instantiate a DB connector. Parameters: Name Type Description Default host str Database host required port str Database port required user str Username required password str Password required allow_local_infile boolean Local infile is allowed when the value is True False Returns: Name Type Description result str Value is 'OK' when successful Source code in honeydew/mysql.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 class MysqlConnector : \"\"\"Instantiate a DB connector. Args: host (str): Database host port (str): Database port user (str): Username password (str): Password allow_local_infile (boolean): Local infile is allowed when the value is True Returns: result (str): Value is 'OK' when successful \"\"\" def __init__ ( self , host , port , user , password , allow_local_infile = False ): self . host = host self . port = port self . user = user self . password = password self . allow_local_infile = allow_local_infile self . db_connection = sql . connect ( host = self . host , port = self . port , user = self . user , password = self . password , ssl_disabled = True , autocommit = True , allow_local_infile = True ) self . db_cursor = self . db_connection . cursor () if allow_local_infile : query_str = \"\"\"SET GLOBAL local_infile=1\"\"\" self . db_cursor . execute ( query_str ) def query_without_fetch ( self , query_str ): \"\"\" Send non DQL query Args: query_str (str): sql query Returns: result (str): Value is 'OK' when successful \"\"\" self . db_cursor . execute ( query_str ) return 'OK' def query_to_dataframe ( self , query_str ): \"\"\" Query and store the result in a dataframe Args: query_str (str): sql query Returns: result (dataframe): Result in a dataframe \"\"\" self . db_cursor . execute ( query_str ) table_rows = self . db_cursor . fetchall () df = pd . DataFrame ( table_rows , columns = self . db_cursor . column_names ) return df def load_csv ( self , db_name , table_name , file_name , write_disposition , delimiter = ',' , ignore_rows = 1 , is_local_csv = True ): \"\"\" Load a local CSV file into a table Args: db_name (str): Database name where the CSV will be loaded table_name (str): Table name where the CSV will be loaded file_name (str): CSV file name delimiter (str): CSV delimiter character ignore_rows (str): Number of rows that will be ignored from the top write_disposition (str): Write method to add data into table (WRITE_TRUNCATE, WRITE_APPEND) is_local_csv (boolean): If the value is True, then CSV file is in local machine. If the value is False, then CSV file is in remote machine. Returns: result (str): The result of function \"\"\" result = '' if write_disposition == 'WRITE_TRUNCATE' : query = 'TRUNCATE TABLE {db_name} . {table_name} ' . format ( db_name = db_name , table_name = table_name ) self . db_cursor . execute ( query ) self . db_connection . commit () # load table if is_local_csv : sql_import_table = ( \"\"\" LOAD DATA LOCAL INFILE ' {file_name} ' INTO TABLE {db_name} . {table_name} FIELDS TERMINATED BY ' {delimiter} ' LINES TERMINATED BY ' \\\\ n' IGNORE {ignore_rows} ROWS; \"\"\" ) . format ( file_name = file_name , db_name = db_name , table_name = table_name , delimiter = delimiter , ignore_rows = ignore_rows ) else : sql_import_table = ( \"\"\" LOAD DATA INFILE ' {file_name} ' INTO TABLE {db_name} . {table_name} FIELDS TERMINATED BY ' {delimiter} ' LINES TERMINATED BY ' \\\\ n' IGNORE {ignore_rows} ROWS; \"\"\" ) . format ( file_name = file_name , db_name = db_name , table_name = table_name , delimiter = delimiter , ignore_rows = ignore_rows ) self . db_cursor . execute ( sql_import_table ) self . db_connection . commit () result = 'OK' return result","title":"MysqlConnector"},{"location":"mysql/#honeydew.mysql.MysqlConnector.load_csv","text":"Load a local CSV file into a table Parameters: Name Type Description Default db_name str Database name where the CSV will be loaded required table_name str Table name where the CSV will be loaded required file_name str CSV file name required delimiter str CSV delimiter character ',' ignore_rows str Number of rows that will be ignored from the top 1 write_disposition str Write method to add data into table (WRITE_TRUNCATE, WRITE_APPEND) required is_local_csv boolean If the value is True, then CSV file is in local machine. If the value is False, then CSV file is in remote machine. True Returns: Name Type Description result str The result of function Source code in honeydew/mysql.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def load_csv ( self , db_name , table_name , file_name , write_disposition , delimiter = ',' , ignore_rows = 1 , is_local_csv = True ): \"\"\" Load a local CSV file into a table Args: db_name (str): Database name where the CSV will be loaded table_name (str): Table name where the CSV will be loaded file_name (str): CSV file name delimiter (str): CSV delimiter character ignore_rows (str): Number of rows that will be ignored from the top write_disposition (str): Write method to add data into table (WRITE_TRUNCATE, WRITE_APPEND) is_local_csv (boolean): If the value is True, then CSV file is in local machine. If the value is False, then CSV file is in remote machine. Returns: result (str): The result of function \"\"\" result = '' if write_disposition == 'WRITE_TRUNCATE' : query = 'TRUNCATE TABLE {db_name} . {table_name} ' . format ( db_name = db_name , table_name = table_name ) self . db_cursor . execute ( query ) self . db_connection . commit () # load table if is_local_csv : sql_import_table = ( \"\"\" LOAD DATA LOCAL INFILE ' {file_name} ' INTO TABLE {db_name} . {table_name} FIELDS TERMINATED BY ' {delimiter} ' LINES TERMINATED BY ' \\\\ n' IGNORE {ignore_rows} ROWS; \"\"\" ) . format ( file_name = file_name , db_name = db_name , table_name = table_name , delimiter = delimiter , ignore_rows = ignore_rows ) else : sql_import_table = ( \"\"\" LOAD DATA INFILE ' {file_name} ' INTO TABLE {db_name} . {table_name} FIELDS TERMINATED BY ' {delimiter} ' LINES TERMINATED BY ' \\\\ n' IGNORE {ignore_rows} ROWS; \"\"\" ) . format ( file_name = file_name , db_name = db_name , table_name = table_name , delimiter = delimiter , ignore_rows = ignore_rows ) self . db_cursor . execute ( sql_import_table ) self . db_connection . commit () result = 'OK' return result","title":"load_csv()"},{"location":"mysql/#honeydew.mysql.MysqlConnector.query_to_dataframe","text":"Query and store the result in a dataframe Parameters: Name Type Description Default query_str str sql query required Returns: Name Type Description result dataframe Result in a dataframe Source code in honeydew/mysql.py 53 54 55 56 57 58 59 60 61 62 63 64 def query_to_dataframe ( self , query_str ): \"\"\" Query and store the result in a dataframe Args: query_str (str): sql query Returns: result (dataframe): Result in a dataframe \"\"\" self . db_cursor . execute ( query_str ) table_rows = self . db_cursor . fetchall () df = pd . DataFrame ( table_rows , columns = self . db_cursor . column_names ) return df","title":"query_to_dataframe()"},{"location":"mysql/#honeydew.mysql.MysqlConnector.query_without_fetch","text":"Send non DQL query Parameters: Name Type Description Default query_str str sql query required Returns: Name Type Description result str Value is 'OK' when successful Source code in honeydew/mysql.py 42 43 44 45 46 47 48 49 50 51 def query_without_fetch ( self , query_str ): \"\"\" Send non DQL query Args: query_str (str): sql query Returns: result (str): Value is 'OK' when successful \"\"\" self . db_cursor . execute ( query_str ) return 'OK'","title":"query_without_fetch()"},{"location":"ssh/","text":"SSH Connector SshConnector Instantiate an SSH connector. Parameters: Name Type Description Default host str SSH host required port str SSH port required private_key str SSH private key file path required username str SSH user required disable_rsa_512_256 boolean If the value is True, then rsa-sha2-512 and rsa-sha2-256 algorithm will be disabled False Source code in honeydew/ssh.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 class SshConnector : \"\"\"Instantiate an SSH connector. Args: host (str): SSH host port (str): SSH port private_key (str): SSH private key file path username (str): SSH user disable_rsa_512_256 (boolean): If the value is True, then rsa-sha2-512 and rsa-sha2-256 algorithm will be disabled \"\"\" def __init__ ( self , host , port , private_key , username , disable_rsa_512_256 = False ): self . host = host self . port = port self . private_key = private_key self . username = username self . disable_rsa_512_256 = disable_rsa_512_256 self . ssh = SSHClient () self . ssh . load_system_host_keys () self . ssh . set_missing_host_key_policy ( paramiko . AutoAddPolicy ()) self . key = paramiko . RSAKey . from_private_key_file ( self . private_key ) if self . disable_rsa_512_256 == False : self . ssh . connect ( self . host , port = self . port , username = self . username , pkey = self . key , timeout = 3600 ) else : self . ssh . connect ( self . host , port = self . port , username = self . username , pkey = self . key , timeout = 3600 , disabled_algorithms = dict ( pubkeys = [ \"rsa-sha2-512\" , \"rsa-sha2-256\" ])) def scp_upload ( self , src , dst ): \"\"\" Upload a file with SCP Args: src (str): Path of source file dst (str): Path of destination file Returns: result (str): The result of function \"\"\" scp = SCPClient ( self . ssh . get_transport ()) scp . put ( src , dst ) return \"\"\" {src} has been uploaded!\"\"\" . format ( src = src ) def scp_download ( self , src , dst ): \"\"\" Download a file with SCP Args: src (str): Path of source file dst (str): Path of destination file Returns: result (str): The result of function \"\"\" scp = SCPClient ( self . ssh . get_transport ()) scp . get ( src , dst ) return \"\"\" {src} has been downloaded!\"\"\" . format ( src = src ) scp_download ( src , dst ) Download a file with SCP Parameters: Name Type Description Default src str Path of source file required dst str Path of destination file required Returns: Name Type Description result str The result of function Source code in honeydew/ssh.py 42 43 44 45 46 47 48 49 50 51 52 53 def scp_download ( self , src , dst ): \"\"\" Download a file with SCP Args: src (str): Path of source file dst (str): Path of destination file Returns: result (str): The result of function \"\"\" scp = SCPClient ( self . ssh . get_transport ()) scp . get ( src , dst ) return \"\"\" {src} has been downloaded!\"\"\" . format ( src = src ) scp_upload ( src , dst ) Upload a file with SCP Parameters: Name Type Description Default src str Path of source file required dst str Path of destination file required Returns: Name Type Description result str The result of function Source code in honeydew/ssh.py 29 30 31 32 33 34 35 36 37 38 39 40 def scp_upload ( self , src , dst ): \"\"\" Upload a file with SCP Args: src (str): Path of source file dst (str): Path of destination file Returns: result (str): The result of function \"\"\" scp = SCPClient ( self . ssh . get_transport ()) scp . put ( src , dst ) return \"\"\" {src} has been uploaded!\"\"\" . format ( src = src )","title":"SSH Connector"},{"location":"ssh/#ssh-connector","text":"","title":"SSH Connector"},{"location":"ssh/#honeydew.ssh.SshConnector","text":"Instantiate an SSH connector. Parameters: Name Type Description Default host str SSH host required port str SSH port required private_key str SSH private key file path required username str SSH user required disable_rsa_512_256 boolean If the value is True, then rsa-sha2-512 and rsa-sha2-256 algorithm will be disabled False Source code in honeydew/ssh.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 class SshConnector : \"\"\"Instantiate an SSH connector. Args: host (str): SSH host port (str): SSH port private_key (str): SSH private key file path username (str): SSH user disable_rsa_512_256 (boolean): If the value is True, then rsa-sha2-512 and rsa-sha2-256 algorithm will be disabled \"\"\" def __init__ ( self , host , port , private_key , username , disable_rsa_512_256 = False ): self . host = host self . port = port self . private_key = private_key self . username = username self . disable_rsa_512_256 = disable_rsa_512_256 self . ssh = SSHClient () self . ssh . load_system_host_keys () self . ssh . set_missing_host_key_policy ( paramiko . AutoAddPolicy ()) self . key = paramiko . RSAKey . from_private_key_file ( self . private_key ) if self . disable_rsa_512_256 == False : self . ssh . connect ( self . host , port = self . port , username = self . username , pkey = self . key , timeout = 3600 ) else : self . ssh . connect ( self . host , port = self . port , username = self . username , pkey = self . key , timeout = 3600 , disabled_algorithms = dict ( pubkeys = [ \"rsa-sha2-512\" , \"rsa-sha2-256\" ])) def scp_upload ( self , src , dst ): \"\"\" Upload a file with SCP Args: src (str): Path of source file dst (str): Path of destination file Returns: result (str): The result of function \"\"\" scp = SCPClient ( self . ssh . get_transport ()) scp . put ( src , dst ) return \"\"\" {src} has been uploaded!\"\"\" . format ( src = src ) def scp_download ( self , src , dst ): \"\"\" Download a file with SCP Args: src (str): Path of source file dst (str): Path of destination file Returns: result (str): The result of function \"\"\" scp = SCPClient ( self . ssh . get_transport ()) scp . get ( src , dst ) return \"\"\" {src} has been downloaded!\"\"\" . format ( src = src )","title":"SshConnector"},{"location":"ssh/#honeydew.ssh.SshConnector.scp_download","text":"Download a file with SCP Parameters: Name Type Description Default src str Path of source file required dst str Path of destination file required Returns: Name Type Description result str The result of function Source code in honeydew/ssh.py 42 43 44 45 46 47 48 49 50 51 52 53 def scp_download ( self , src , dst ): \"\"\" Download a file with SCP Args: src (str): Path of source file dst (str): Path of destination file Returns: result (str): The result of function \"\"\" scp = SCPClient ( self . ssh . get_transport ()) scp . get ( src , dst ) return \"\"\" {src} has been downloaded!\"\"\" . format ( src = src )","title":"scp_download()"},{"location":"ssh/#honeydew.ssh.SshConnector.scp_upload","text":"Upload a file with SCP Parameters: Name Type Description Default src str Path of source file required dst str Path of destination file required Returns: Name Type Description result str The result of function Source code in honeydew/ssh.py 29 30 31 32 33 34 35 36 37 38 39 40 def scp_upload ( self , src , dst ): \"\"\" Upload a file with SCP Args: src (str): Path of source file dst (str): Path of destination file Returns: result (str): The result of function \"\"\" scp = SCPClient ( self . ssh . get_transport ()) scp . put ( src , dst ) return \"\"\" {src} has been uploaded!\"\"\" . format ( src = src )","title":"scp_upload()"},{"location":"utils/","text":"Utility Functions Utils Instantiate utilities. Source code in honeydew/utils.py 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class Utils : \"\"\"Instantiate utilities. \"\"\" def __init__ ( self , name = '' ): \"\"\"Instantiate utilities. \"\"\" def convert_dt_to_epoch ( self , dt ): \"\"\" Convert datetime in UTC time zone to epoch (unix time) Args: dt (datetime): datetime Returns: result (int): epoch or unix time \"\"\" return calendar . timegm ( dt . utctimetuple ()) def convert_epoch_to_dt ( self , epoch ): \"\"\" Convert epoch to datetime Args: epoch (int): epoch or unix time Returns: result (datetime): datetime \"\"\" if epoch > 9999999999 : epoch = round ( epoch / 1000 ) return datetime . fromtimestamp ( s ) __init__ ( name = '' ) Instantiate utilities. Source code in honeydew/utils.py 6 7 8 def __init__ ( self , name = '' ): \"\"\"Instantiate utilities. \"\"\" convert_dt_to_epoch ( dt ) Convert datetime in UTC time zone to epoch (unix time) Parameters: Name Type Description Default dt datetime datetime required Returns: Name Type Description result int epoch or unix time Source code in honeydew/utils.py 10 11 12 13 14 15 16 17 18 def convert_dt_to_epoch ( self , dt ): \"\"\" Convert datetime in UTC time zone to epoch (unix time) Args: dt (datetime): datetime Returns: result (int): epoch or unix time \"\"\" return calendar . timegm ( dt . utctimetuple ()) convert_epoch_to_dt ( epoch ) Convert epoch to datetime Parameters: Name Type Description Default epoch int epoch or unix time required Returns: Name Type Description result datetime datetime Source code in honeydew/utils.py 20 21 22 23 24 25 26 27 28 29 30 def convert_epoch_to_dt ( self , epoch ): \"\"\" Convert epoch to datetime Args: epoch (int): epoch or unix time Returns: result (datetime): datetime \"\"\" if epoch > 9999999999 : epoch = round ( epoch / 1000 ) return datetime . fromtimestamp ( s )","title":"Utility Functions"},{"location":"utils/#utility-functions","text":"","title":"Utility Functions"},{"location":"utils/#honeydew.utils.Utils","text":"Instantiate utilities. Source code in honeydew/utils.py 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class Utils : \"\"\"Instantiate utilities. \"\"\" def __init__ ( self , name = '' ): \"\"\"Instantiate utilities. \"\"\" def convert_dt_to_epoch ( self , dt ): \"\"\" Convert datetime in UTC time zone to epoch (unix time) Args: dt (datetime): datetime Returns: result (int): epoch or unix time \"\"\" return calendar . timegm ( dt . utctimetuple ()) def convert_epoch_to_dt ( self , epoch ): \"\"\" Convert epoch to datetime Args: epoch (int): epoch or unix time Returns: result (datetime): datetime \"\"\" if epoch > 9999999999 : epoch = round ( epoch / 1000 ) return datetime . fromtimestamp ( s )","title":"Utils"},{"location":"utils/#honeydew.utils.Utils.__init__","text":"Instantiate utilities. Source code in honeydew/utils.py 6 7 8 def __init__ ( self , name = '' ): \"\"\"Instantiate utilities. \"\"\"","title":"__init__()"},{"location":"utils/#honeydew.utils.Utils.convert_dt_to_epoch","text":"Convert datetime in UTC time zone to epoch (unix time) Parameters: Name Type Description Default dt datetime datetime required Returns: Name Type Description result int epoch or unix time Source code in honeydew/utils.py 10 11 12 13 14 15 16 17 18 def convert_dt_to_epoch ( self , dt ): \"\"\" Convert datetime in UTC time zone to epoch (unix time) Args: dt (datetime): datetime Returns: result (int): epoch or unix time \"\"\" return calendar . timegm ( dt . utctimetuple ())","title":"convert_dt_to_epoch()"},{"location":"utils/#honeydew.utils.Utils.convert_epoch_to_dt","text":"Convert epoch to datetime Parameters: Name Type Description Default epoch int epoch or unix time required Returns: Name Type Description result datetime datetime Source code in honeydew/utils.py 20 21 22 23 24 25 26 27 28 29 30 def convert_epoch_to_dt ( self , epoch ): \"\"\" Convert epoch to datetime Args: epoch (int): epoch or unix time Returns: result (datetime): datetime \"\"\" if epoch > 9999999999 : epoch = round ( epoch / 1000 ) return datetime . fromtimestamp ( s )","title":"convert_epoch_to_dt()"}]}