{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Honeydew Documentation Honeydew library is a collection of ETL functions. Feel free to use and have fun with your next ETL workflows! Supported connectors: MySQL GCP Installation To install honeydew, run the following command from the command line: pip install honeydew --upgrade Repository Check out the source code here !","title":"Honeydew Docs"},{"location":"#welcome-to-honeydew-documentation","text":"Honeydew library is a collection of ETL functions. Feel free to use and have fun with your next ETL workflows!","title":"Welcome to Honeydew Documentation"},{"location":"#supported-connectors","text":"MySQL GCP","title":"Supported connectors:"},{"location":"#installation","text":"To install honeydew, run the following command from the command line: pip install honeydew --upgrade","title":"Installation"},{"location":"#repository","text":"Check out the source code here !","title":"Repository"},{"location":"gcp/","text":"GCP Connector GcpConnector Instantiate a GCP connector. Parameters: Name Type Description Default credential_file str Credential json file required proxy str Proxy address '' Source code in honeydew/gcp.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 class GcpConnector : \"\"\" Instantiate a GCP connector. Args: credential_file (str): Credential json file proxy (str): Proxy address \"\"\" def __init__ ( self , credential_file , proxy = '' ): self . credential_file = credential_file self . proxy = proxy os . environ [ \"GOOGLE_APPLICATION_CREDENTIALS\" ] = credential_file if proxy != '' : os . environ [ 'HTTP_PROXY' ] = proxy os . environ [ 'HTTPS_PROXY' ] = proxy def bq_query_to_dataframe ( self , project_id , query , timeout = 3600 , method = 1 ): \"\"\" Submit query to BigQuery and store result into pandas dataframe Args: project_id (str): Project ID query (str): SQL query timeout (int): Query timeout in seconds method (int): API that will be used to query (1: google-cloud-bigquery, 2: pandas-gbq) Returns: result (dataframe)): Result in pandas dataframe \"\"\" df = pd . DataFrame () bqclient = bigquery . Client ( project = project_id ) query_job = bqclient . query ( query ) if method == 2 : df = pd . read_gbq ( query = query , project_id = project_id ) else : rows = list ( query_job . result ( timeout = timeout )) df = pd . DataFrame ( data = [ list ( x . values ()) for x in rows ], columns = list ( rows [ 0 ] . keys ())) return df def bq_query_non_dql ( self , project_id , query ): \"\"\" Submit non Data Query Language (DQL) type of query to BigQuery. Example: CREATE, DROP, TRUNCATE, INSERT, UPDATE, DELETE Args: project_id (str): Project ID query (str): SQL query Returns: result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result \"\"\" bqclient = bigquery . Client ( project = project_id ) query_job = bqclient . query ( query ) results = query_job . result () return results def bq_export_table_to_gcs ( self , project_id , dataset_id , table_id , gcs_uri , format = 'CSV' , delimiter = ',' , enable_compression = True , compression = 'GZIP' , overwrite = True , region = 'northamerica-northeast1' ): \"\"\" Export BigQuery table into Google Cloud Storage (GCS) Args: project_id (str): Project ID table_id (str): Table ID dataset_id (str): Dataset ID gcs_uri (str): GCS URI as destination. Example: 'gs://my-bucket/my-dir/tickets-20220101-*.csv.gz' format (str): File format (CSV, JSON, Avro, Parquet). Default: 'CSV' delimiter (str): CSV delimiter character. Default: ',' enable_compression (boolean): Files will be compressed if the value is True. Default: True compression (str): Compression format. Default: GZIP. Reference: https://cloud.google.com/bigquery/docs/exporting-data#export_formats_and_compression_types overwrite (boolean): GCS URI destination will be overwritten if the value is True. Default: True region (str): Region to run the process. Default: 'northamerica-northeast1' Returns: result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result \"\"\" bqclient = bigquery . Client ( project = project_id ) dataset_ref = bigquery . DatasetReference ( project_id , dataset_id ) table_ref = dataset_ref . table ( table_id ) job_config = bigquery . job . ExtractJobConfig () if enable_compression == True : if compression == 'DEFLATE' : job_config . compression = bigquery . Compression . DEFLATE if compression == 'SNAPPY' : job_config . compression = bigquery . Compression . SNAPPY else : job_config . compression = bigquery . Compression . GZIP extract_job = bqclient . extract_table ( table_ref , gcs_uri , location = region , job_config = job_config ) results = extract_job . result () return results def gcs_download_single_file ( self , project_id , bucket_id , source_blob_path , destination_path ): \"\"\" Download a single object from Google Cloud Storage (GCS) Args: project_id (str): Project ID bucket_id (str): Bucket ID source_blob_path (str): The path of source object. Example: 'gcs-directory/my-filename.txt' destination_path (str): Local destination path. Example: '/my-directory/my-filename.txt' Returns: result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result \"\"\" gcs_client = storage . Client ( project = project_id ) bucket = gcs_client . bucket ( bucket_id ) blob = bucket . blob ( source_blob_path ) blob . download_to_filename ( destination_path ) results = 'OK' return results def gcs_download_objects_with_pattern ( self , project_id , bucket_id , blob_prefix , destination_dir_path , printout = True ): \"\"\" Download multiple objects which have same prefix pattern from Google Cloud Storage (GCS) Args: project_id (str): Project ID bucket_id (str): Bucket ID blob_prefix (str): The blob prefix pattern that wil be downloaded. Example: 'gcs-directory/tickets-20220101-' destination_dir_path (str): Local destination directory path. Example: '/my-directory' printout (boolean): File name will be displayed if this value is true. Default: True \"\"\" delimiter = '/' storage_client = storage . Client ( project_id ) bucket = storage_client . get_bucket ( bucket_id ) # List blobs iterate in folder blobs = bucket . list_blobs ( prefix = blob_prefix , delimiter = delimiter ) # Excluding folder inside bucket for blob in blobs : if printout == True : print ( blob . name ) filename_raw = blob . name . split ( '/' ) filename = filename_raw [ len ( filename_raw ) - 1 ] destination_uri = ' {}{} ' . format ( destination_dir_path , filename ) blob . download_to_filename ( destination_uri ) results = 'OK' return results bq_export_table_to_gcs ( project_id , dataset_id , table_id , gcs_uri , format = 'CSV' , delimiter = ',' , enable_compression = True , compression = 'GZIP' , overwrite = True , region = 'northamerica-northeast1' ) Export BigQuery table into Google Cloud Storage (GCS) Parameters: Name Type Description Default project_id str Project ID required table_id str Table ID required dataset_id str Dataset ID required gcs_uri str GCS URI as destination. Example: 'gs://my-bucket/my-dir/tickets-20220101-*.csv.gz' required format str File format (CSV, JSON, Avro, Parquet). Default: 'CSV' 'CSV' delimiter str CSV delimiter character. Default: ',' ',' enable_compression boolean Files will be compressed if the value is True. Default: True True compression str Compression format. Default: GZIP. Reference: https://cloud.google.com/bigquery/docs/exporting-data#export_formats_and_compression_types 'GZIP' overwrite boolean GCS URI destination will be overwritten if the value is True. Default: True True region str Region to run the process. Default: 'northamerica-northeast1' 'northamerica-northeast1' Returns: Name Type Description result result Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result Source code in honeydew/gcp.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def bq_export_table_to_gcs ( self , project_id , dataset_id , table_id , gcs_uri , format = 'CSV' , delimiter = ',' , enable_compression = True , compression = 'GZIP' , overwrite = True , region = 'northamerica-northeast1' ): \"\"\" Export BigQuery table into Google Cloud Storage (GCS) Args: project_id (str): Project ID table_id (str): Table ID dataset_id (str): Dataset ID gcs_uri (str): GCS URI as destination. Example: 'gs://my-bucket/my-dir/tickets-20220101-*.csv.gz' format (str): File format (CSV, JSON, Avro, Parquet). Default: 'CSV' delimiter (str): CSV delimiter character. Default: ',' enable_compression (boolean): Files will be compressed if the value is True. Default: True compression (str): Compression format. Default: GZIP. Reference: https://cloud.google.com/bigquery/docs/exporting-data#export_formats_and_compression_types overwrite (boolean): GCS URI destination will be overwritten if the value is True. Default: True region (str): Region to run the process. Default: 'northamerica-northeast1' Returns: result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result \"\"\" bqclient = bigquery . Client ( project = project_id ) dataset_ref = bigquery . DatasetReference ( project_id , dataset_id ) table_ref = dataset_ref . table ( table_id ) job_config = bigquery . job . ExtractJobConfig () if enable_compression == True : if compression == 'DEFLATE' : job_config . compression = bigquery . Compression . DEFLATE if compression == 'SNAPPY' : job_config . compression = bigquery . Compression . SNAPPY else : job_config . compression = bigquery . Compression . GZIP extract_job = bqclient . extract_table ( table_ref , gcs_uri , location = region , job_config = job_config ) results = extract_job . result () return results bq_query_non_dql ( project_id , query ) Submit non Data Query Language (DQL) type of query to BigQuery. Example: CREATE, DROP, TRUNCATE, INSERT, UPDATE, DELETE Parameters: Name Type Description Default project_id str Project ID required query str SQL query required Returns: Name Type Description result result Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result Source code in honeydew/gcp.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def bq_query_non_dql ( self , project_id , query ): \"\"\" Submit non Data Query Language (DQL) type of query to BigQuery. Example: CREATE, DROP, TRUNCATE, INSERT, UPDATE, DELETE Args: project_id (str): Project ID query (str): SQL query Returns: result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result \"\"\" bqclient = bigquery . Client ( project = project_id ) query_job = bqclient . query ( query ) results = query_job . result () return results bq_query_to_dataframe ( project_id , query , timeout = 3600 , method = 1 ) Submit query to BigQuery and store result into pandas dataframe Parameters: Name Type Description Default project_id str Project ID required query str SQL query required timeout int Query timeout in seconds 3600 method int API that will be used to query (1: google-cloud-bigquery, 2: pandas-gbq) 1 Returns: Name Type Description result dataframe) Result in pandas dataframe Source code in honeydew/gcp.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def bq_query_to_dataframe ( self , project_id , query , timeout = 3600 , method = 1 ): \"\"\" Submit query to BigQuery and store result into pandas dataframe Args: project_id (str): Project ID query (str): SQL query timeout (int): Query timeout in seconds method (int): API that will be used to query (1: google-cloud-bigquery, 2: pandas-gbq) Returns: result (dataframe)): Result in pandas dataframe \"\"\" df = pd . DataFrame () bqclient = bigquery . Client ( project = project_id ) query_job = bqclient . query ( query ) if method == 2 : df = pd . read_gbq ( query = query , project_id = project_id ) else : rows = list ( query_job . result ( timeout = timeout )) df = pd . DataFrame ( data = [ list ( x . values ()) for x in rows ], columns = list ( rows [ 0 ] . keys ())) return df gcs_download_objects_with_pattern ( project_id , bucket_id , blob_prefix , destination_dir_path , printout = True ) Download multiple objects which have same prefix pattern from Google Cloud Storage (GCS) Parameters: Name Type Description Default project_id str Project ID required bucket_id str Bucket ID required blob_prefix str The blob prefix pattern that wil be downloaded. Example: 'gcs-directory/tickets-20220101-' required destination_dir_path str Local destination directory path. Example: '/my-directory' required printout boolean File name will be displayed if this value is true. Default: True True Source code in honeydew/gcp.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def gcs_download_objects_with_pattern ( self , project_id , bucket_id , blob_prefix , destination_dir_path , printout = True ): \"\"\" Download multiple objects which have same prefix pattern from Google Cloud Storage (GCS) Args: project_id (str): Project ID bucket_id (str): Bucket ID blob_prefix (str): The blob prefix pattern that wil be downloaded. Example: 'gcs-directory/tickets-20220101-' destination_dir_path (str): Local destination directory path. Example: '/my-directory' printout (boolean): File name will be displayed if this value is true. Default: True \"\"\" delimiter = '/' storage_client = storage . Client ( project_id ) bucket = storage_client . get_bucket ( bucket_id ) # List blobs iterate in folder blobs = bucket . list_blobs ( prefix = blob_prefix , delimiter = delimiter ) # Excluding folder inside bucket for blob in blobs : if printout == True : print ( blob . name ) filename_raw = blob . name . split ( '/' ) filename = filename_raw [ len ( filename_raw ) - 1 ] destination_uri = ' {}{} ' . format ( destination_dir_path , filename ) blob . download_to_filename ( destination_uri ) results = 'OK' return results gcs_download_single_file ( project_id , bucket_id , source_blob_path , destination_path ) Download a single object from Google Cloud Storage (GCS) Parameters: Name Type Description Default project_id str Project ID required bucket_id str Bucket ID required source_blob_path str The path of source object. Example: 'gcs-directory/my-filename.txt' required destination_path str Local destination path. Example: '/my-directory/my-filename.txt' required Returns: Name Type Description result result Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result Source code in honeydew/gcp.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def gcs_download_single_file ( self , project_id , bucket_id , source_blob_path , destination_path ): \"\"\" Download a single object from Google Cloud Storage (GCS) Args: project_id (str): Project ID bucket_id (str): Bucket ID source_blob_path (str): The path of source object. Example: 'gcs-directory/my-filename.txt' destination_path (str): Local destination path. Example: '/my-directory/my-filename.txt' Returns: result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result \"\"\" gcs_client = storage . Client ( project = project_id ) bucket = gcs_client . bucket ( bucket_id ) blob = bucket . blob ( source_blob_path ) blob . download_to_filename ( destination_path ) results = 'OK' return results","title":"GCP Connector"},{"location":"gcp/#gcp-connector","text":"","title":"GCP Connector"},{"location":"gcp/#honeydew.gcp.GcpConnector","text":"Instantiate a GCP connector. Parameters: Name Type Description Default credential_file str Credential json file required proxy str Proxy address '' Source code in honeydew/gcp.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 class GcpConnector : \"\"\" Instantiate a GCP connector. Args: credential_file (str): Credential json file proxy (str): Proxy address \"\"\" def __init__ ( self , credential_file , proxy = '' ): self . credential_file = credential_file self . proxy = proxy os . environ [ \"GOOGLE_APPLICATION_CREDENTIALS\" ] = credential_file if proxy != '' : os . environ [ 'HTTP_PROXY' ] = proxy os . environ [ 'HTTPS_PROXY' ] = proxy def bq_query_to_dataframe ( self , project_id , query , timeout = 3600 , method = 1 ): \"\"\" Submit query to BigQuery and store result into pandas dataframe Args: project_id (str): Project ID query (str): SQL query timeout (int): Query timeout in seconds method (int): API that will be used to query (1: google-cloud-bigquery, 2: pandas-gbq) Returns: result (dataframe)): Result in pandas dataframe \"\"\" df = pd . DataFrame () bqclient = bigquery . Client ( project = project_id ) query_job = bqclient . query ( query ) if method == 2 : df = pd . read_gbq ( query = query , project_id = project_id ) else : rows = list ( query_job . result ( timeout = timeout )) df = pd . DataFrame ( data = [ list ( x . values ()) for x in rows ], columns = list ( rows [ 0 ] . keys ())) return df def bq_query_non_dql ( self , project_id , query ): \"\"\" Submit non Data Query Language (DQL) type of query to BigQuery. Example: CREATE, DROP, TRUNCATE, INSERT, UPDATE, DELETE Args: project_id (str): Project ID query (str): SQL query Returns: result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result \"\"\" bqclient = bigquery . Client ( project = project_id ) query_job = bqclient . query ( query ) results = query_job . result () return results def bq_export_table_to_gcs ( self , project_id , dataset_id , table_id , gcs_uri , format = 'CSV' , delimiter = ',' , enable_compression = True , compression = 'GZIP' , overwrite = True , region = 'northamerica-northeast1' ): \"\"\" Export BigQuery table into Google Cloud Storage (GCS) Args: project_id (str): Project ID table_id (str): Table ID dataset_id (str): Dataset ID gcs_uri (str): GCS URI as destination. Example: 'gs://my-bucket/my-dir/tickets-20220101-*.csv.gz' format (str): File format (CSV, JSON, Avro, Parquet). Default: 'CSV' delimiter (str): CSV delimiter character. Default: ',' enable_compression (boolean): Files will be compressed if the value is True. Default: True compression (str): Compression format. Default: GZIP. Reference: https://cloud.google.com/bigquery/docs/exporting-data#export_formats_and_compression_types overwrite (boolean): GCS URI destination will be overwritten if the value is True. Default: True region (str): Region to run the process. Default: 'northamerica-northeast1' Returns: result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result \"\"\" bqclient = bigquery . Client ( project = project_id ) dataset_ref = bigquery . DatasetReference ( project_id , dataset_id ) table_ref = dataset_ref . table ( table_id ) job_config = bigquery . job . ExtractJobConfig () if enable_compression == True : if compression == 'DEFLATE' : job_config . compression = bigquery . Compression . DEFLATE if compression == 'SNAPPY' : job_config . compression = bigquery . Compression . SNAPPY else : job_config . compression = bigquery . Compression . GZIP extract_job = bqclient . extract_table ( table_ref , gcs_uri , location = region , job_config = job_config ) results = extract_job . result () return results def gcs_download_single_file ( self , project_id , bucket_id , source_blob_path , destination_path ): \"\"\" Download a single object from Google Cloud Storage (GCS) Args: project_id (str): Project ID bucket_id (str): Bucket ID source_blob_path (str): The path of source object. Example: 'gcs-directory/my-filename.txt' destination_path (str): Local destination path. Example: '/my-directory/my-filename.txt' Returns: result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result \"\"\" gcs_client = storage . Client ( project = project_id ) bucket = gcs_client . bucket ( bucket_id ) blob = bucket . blob ( source_blob_path ) blob . download_to_filename ( destination_path ) results = 'OK' return results def gcs_download_objects_with_pattern ( self , project_id , bucket_id , blob_prefix , destination_dir_path , printout = True ): \"\"\" Download multiple objects which have same prefix pattern from Google Cloud Storage (GCS) Args: project_id (str): Project ID bucket_id (str): Bucket ID blob_prefix (str): The blob prefix pattern that wil be downloaded. Example: 'gcs-directory/tickets-20220101-' destination_dir_path (str): Local destination directory path. Example: '/my-directory' printout (boolean): File name will be displayed if this value is true. Default: True \"\"\" delimiter = '/' storage_client = storage . Client ( project_id ) bucket = storage_client . get_bucket ( bucket_id ) # List blobs iterate in folder blobs = bucket . list_blobs ( prefix = blob_prefix , delimiter = delimiter ) # Excluding folder inside bucket for blob in blobs : if printout == True : print ( blob . name ) filename_raw = blob . name . split ( '/' ) filename = filename_raw [ len ( filename_raw ) - 1 ] destination_uri = ' {}{} ' . format ( destination_dir_path , filename ) blob . download_to_filename ( destination_uri ) results = 'OK' return results","title":"GcpConnector"},{"location":"gcp/#honeydew.gcp.GcpConnector.bq_export_table_to_gcs","text":"Export BigQuery table into Google Cloud Storage (GCS) Parameters: Name Type Description Default project_id str Project ID required table_id str Table ID required dataset_id str Dataset ID required gcs_uri str GCS URI as destination. Example: 'gs://my-bucket/my-dir/tickets-20220101-*.csv.gz' required format str File format (CSV, JSON, Avro, Parquet). Default: 'CSV' 'CSV' delimiter str CSV delimiter character. Default: ',' ',' enable_compression boolean Files will be compressed if the value is True. Default: True True compression str Compression format. Default: GZIP. Reference: https://cloud.google.com/bigquery/docs/exporting-data#export_formats_and_compression_types 'GZIP' overwrite boolean GCS URI destination will be overwritten if the value is True. Default: True True region str Region to run the process. Default: 'northamerica-northeast1' 'northamerica-northeast1' Returns: Name Type Description result result Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result Source code in honeydew/gcp.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def bq_export_table_to_gcs ( self , project_id , dataset_id , table_id , gcs_uri , format = 'CSV' , delimiter = ',' , enable_compression = True , compression = 'GZIP' , overwrite = True , region = 'northamerica-northeast1' ): \"\"\" Export BigQuery table into Google Cloud Storage (GCS) Args: project_id (str): Project ID table_id (str): Table ID dataset_id (str): Dataset ID gcs_uri (str): GCS URI as destination. Example: 'gs://my-bucket/my-dir/tickets-20220101-*.csv.gz' format (str): File format (CSV, JSON, Avro, Parquet). Default: 'CSV' delimiter (str): CSV delimiter character. Default: ',' enable_compression (boolean): Files will be compressed if the value is True. Default: True compression (str): Compression format. Default: GZIP. Reference: https://cloud.google.com/bigquery/docs/exporting-data#export_formats_and_compression_types overwrite (boolean): GCS URI destination will be overwritten if the value is True. Default: True region (str): Region to run the process. Default: 'northamerica-northeast1' Returns: result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result \"\"\" bqclient = bigquery . Client ( project = project_id ) dataset_ref = bigquery . DatasetReference ( project_id , dataset_id ) table_ref = dataset_ref . table ( table_id ) job_config = bigquery . job . ExtractJobConfig () if enable_compression == True : if compression == 'DEFLATE' : job_config . compression = bigquery . Compression . DEFLATE if compression == 'SNAPPY' : job_config . compression = bigquery . Compression . SNAPPY else : job_config . compression = bigquery . Compression . GZIP extract_job = bqclient . extract_table ( table_ref , gcs_uri , location = region , job_config = job_config ) results = extract_job . result () return results","title":"bq_export_table_to_gcs()"},{"location":"gcp/#honeydew.gcp.GcpConnector.bq_query_non_dql","text":"Submit non Data Query Language (DQL) type of query to BigQuery. Example: CREATE, DROP, TRUNCATE, INSERT, UPDATE, DELETE Parameters: Name Type Description Default project_id str Project ID required query str SQL query required Returns: Name Type Description result result Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result Source code in honeydew/gcp.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def bq_query_non_dql ( self , project_id , query ): \"\"\" Submit non Data Query Language (DQL) type of query to BigQuery. Example: CREATE, DROP, TRUNCATE, INSERT, UPDATE, DELETE Args: project_id (str): Project ID query (str): SQL query Returns: result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result \"\"\" bqclient = bigquery . Client ( project = project_id ) query_job = bqclient . query ( query ) results = query_job . result () return results","title":"bq_query_non_dql()"},{"location":"gcp/#honeydew.gcp.GcpConnector.bq_query_to_dataframe","text":"Submit query to BigQuery and store result into pandas dataframe Parameters: Name Type Description Default project_id str Project ID required query str SQL query required timeout int Query timeout in seconds 3600 method int API that will be used to query (1: google-cloud-bigquery, 2: pandas-gbq) 1 Returns: Name Type Description result dataframe) Result in pandas dataframe Source code in honeydew/gcp.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def bq_query_to_dataframe ( self , project_id , query , timeout = 3600 , method = 1 ): \"\"\" Submit query to BigQuery and store result into pandas dataframe Args: project_id (str): Project ID query (str): SQL query timeout (int): Query timeout in seconds method (int): API that will be used to query (1: google-cloud-bigquery, 2: pandas-gbq) Returns: result (dataframe)): Result in pandas dataframe \"\"\" df = pd . DataFrame () bqclient = bigquery . Client ( project = project_id ) query_job = bqclient . query ( query ) if method == 2 : df = pd . read_gbq ( query = query , project_id = project_id ) else : rows = list ( query_job . result ( timeout = timeout )) df = pd . DataFrame ( data = [ list ( x . values ()) for x in rows ], columns = list ( rows [ 0 ] . keys ())) return df","title":"bq_query_to_dataframe()"},{"location":"gcp/#honeydew.gcp.GcpConnector.gcs_download_objects_with_pattern","text":"Download multiple objects which have same prefix pattern from Google Cloud Storage (GCS) Parameters: Name Type Description Default project_id str Project ID required bucket_id str Bucket ID required blob_prefix str The blob prefix pattern that wil be downloaded. Example: 'gcs-directory/tickets-20220101-' required destination_dir_path str Local destination directory path. Example: '/my-directory' required printout boolean File name will be displayed if this value is true. Default: True True Source code in honeydew/gcp.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def gcs_download_objects_with_pattern ( self , project_id , bucket_id , blob_prefix , destination_dir_path , printout = True ): \"\"\" Download multiple objects which have same prefix pattern from Google Cloud Storage (GCS) Args: project_id (str): Project ID bucket_id (str): Bucket ID blob_prefix (str): The blob prefix pattern that wil be downloaded. Example: 'gcs-directory/tickets-20220101-' destination_dir_path (str): Local destination directory path. Example: '/my-directory' printout (boolean): File name will be displayed if this value is true. Default: True \"\"\" delimiter = '/' storage_client = storage . Client ( project_id ) bucket = storage_client . get_bucket ( bucket_id ) # List blobs iterate in folder blobs = bucket . list_blobs ( prefix = blob_prefix , delimiter = delimiter ) # Excluding folder inside bucket for blob in blobs : if printout == True : print ( blob . name ) filename_raw = blob . name . split ( '/' ) filename = filename_raw [ len ( filename_raw ) - 1 ] destination_uri = ' {}{} ' . format ( destination_dir_path , filename ) blob . download_to_filename ( destination_uri ) results = 'OK' return results","title":"gcs_download_objects_with_pattern()"},{"location":"gcp/#honeydew.gcp.GcpConnector.gcs_download_single_file","text":"Download a single object from Google Cloud Storage (GCS) Parameters: Name Type Description Default project_id str Project ID required bucket_id str Bucket ID required source_blob_path str The path of source object. Example: 'gcs-directory/my-filename.txt' required destination_path str Local destination path. Example: '/my-directory/my-filename.txt' required Returns: Name Type Description result result Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result Source code in honeydew/gcp.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def gcs_download_single_file ( self , project_id , bucket_id , source_blob_path , destination_path ): \"\"\" Download a single object from Google Cloud Storage (GCS) Args: project_id (str): Project ID bucket_id (str): Bucket ID source_blob_path (str): The path of source object. Example: 'gcs-directory/my-filename.txt' destination_path (str): Local destination path. Example: '/my-directory/my-filename.txt' Returns: result (result): Iterator of row data. Reference: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=job%20result#google.cloud.bigquery.job.QueryJob.result \"\"\" gcs_client = storage . Client ( project = project_id ) bucket = gcs_client . bucket ( bucket_id ) blob = bucket . blob ( source_blob_path ) blob . download_to_filename ( destination_path ) results = 'OK' return results","title":"gcs_download_single_file()"},{"location":"how-to-guides/","text":"MySQL How to import a CSV file into MySQL by replacing a table from honeydew import mysql_connector # Instantiate a mysql connector mysql_conn = mysql_connector(host = 'mysql.mydomain.com', port = '3306', user = 'my_user', password = 'my_password') # Import a CSV file into a table by replacing the content result = mysql_conn.load_csv_local( db_name='db_name', table_name='table_name', file_name='test.csv', write_disposition='WRITE_TRUNCATE' ) How to import a CSV file into MySQL by appending a table from honeydew import db_connector # Instantiate a mysql connector mysql_conn = db_connector(db_type = 'mysql', host = 'mysql.mydomain.com', port = '3306', user = 'my_user', password = 'my_password') # Import a CSV file into a table by replacing the content result = mysql_conn.load_csv_local( db_name='db_name', table_name='table_name', file_name='test.csv', write_disposition='WRITE_APPEND' ) GCP How to instantiate a GCP connector with or without proxy from honeydew import gcp_connector # Instantiate a GCP connector with internet connection behind proxy g_client = gcp_connector(credential_file='my-secret-credential.json', proxy='http://proxy.mydomain.com:8080') # Instantiate a GCP connector with direct internet connection g_client = gcp_connector(credential_file='my-secret-credential.json') How to query BigQuery table and store the result into a DataFrame from honeydew import gcp_connector # Instantiate a GCP connector with internet connection behind proxy g_client = gcp_connector(credential_file='my-secret-credential.json', proxy='http://proxy.mydomain.com:8080') project_id = 'sweet-honeydew-125283' # Submit a query and store the result into a DataFrame query = 'SELECT * FROM `sweet-honeydew-125283.universe.galaxies` LIMIT 5' df = g_client.bq_query_to_dataframe(project_id, query) print(df.head())","title":"How-To Guides"},{"location":"how-to-guides/#mysql","text":"","title":"MySQL"},{"location":"how-to-guides/#how-to-import-a-csv-file-into-mysql-by-replacing-a-table","text":"from honeydew import mysql_connector # Instantiate a mysql connector mysql_conn = mysql_connector(host = 'mysql.mydomain.com', port = '3306', user = 'my_user', password = 'my_password') # Import a CSV file into a table by replacing the content result = mysql_conn.load_csv_local( db_name='db_name', table_name='table_name', file_name='test.csv', write_disposition='WRITE_TRUNCATE' )","title":"How to import a CSV file into MySQL by replacing a table"},{"location":"how-to-guides/#how-to-import-a-csv-file-into-mysql-by-appending-a-table","text":"from honeydew import db_connector # Instantiate a mysql connector mysql_conn = db_connector(db_type = 'mysql', host = 'mysql.mydomain.com', port = '3306', user = 'my_user', password = 'my_password') # Import a CSV file into a table by replacing the content result = mysql_conn.load_csv_local( db_name='db_name', table_name='table_name', file_name='test.csv', write_disposition='WRITE_APPEND' )","title":"How to import a CSV file into MySQL by appending a table"},{"location":"how-to-guides/#gcp","text":"","title":"GCP"},{"location":"how-to-guides/#how-to-instantiate-a-gcp-connector-with-or-without-proxy","text":"from honeydew import gcp_connector # Instantiate a GCP connector with internet connection behind proxy g_client = gcp_connector(credential_file='my-secret-credential.json', proxy='http://proxy.mydomain.com:8080') # Instantiate a GCP connector with direct internet connection g_client = gcp_connector(credential_file='my-secret-credential.json')","title":"How to instantiate a GCP connector with or without proxy"},{"location":"how-to-guides/#how-to-query-bigquery-table-and-store-the-result-into-a-dataframe","text":"from honeydew import gcp_connector # Instantiate a GCP connector with internet connection behind proxy g_client = gcp_connector(credential_file='my-secret-credential.json', proxy='http://proxy.mydomain.com:8080') project_id = 'sweet-honeydew-125283' # Submit a query and store the result into a DataFrame query = 'SELECT * FROM `sweet-honeydew-125283.universe.galaxies` LIMIT 5' df = g_client.bq_query_to_dataframe(project_id, query) print(df.head())","title":"How to query BigQuery table and store the result into a DataFrame"},{"location":"mysql/","text":"MySQL Connector MysqlConnector Source code in honeydew/mysql.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 class MysqlConnector : def __init__ ( self , host , port , user , password , allow_local_infile = False ): \"\"\"Instantiate a DB connector. Args: host (str): Database host port (str): Database port user (str): Username password (str): Password allow_local_infile (boolean): Local infile is allowed when the value is True Returns: result (str): Value is 'OK' when successful \"\"\" self . host = host self . port = port self . user = user self . password = password self . allow_local_infile = allow_local_infile self . db_connection = sql . connect ( host = self . host , port = self . port , user = self . user , password = self . password , ssl_disabled = True , autocommit = True , allow_local_infile = True ) self . db_cursor = self . db_connection . cursor () if allow_local_infile : query_str = \"\"\"SET GLOBAL local_infile=1\"\"\" self . db_cursor . execute ( query_str ) def query_without_fetch ( self , query_str ): \"\"\" Send non DQL query Args: query_str (str): sql query Returns: result (str): Value is 'OK' when successful \"\"\" self . db_cursor . execute ( query_str ) return 'OK' def query_to_dataframe ( self , query_str ): \"\"\" Query and store the result in a dataframe Args: query_str (str): sql query Returns: result (dataframe): Result in a dataframe \"\"\" self . db_cursor . execute ( query_str ) table_rows = self . db_cursor . fetchall () df = pd . DataFrame ( table_rows , columns = self . db_cursor . column_names ) return df def load_csv ( self , db_name , table_name , file_name , write_disposition , delimiter = ',' , ignore_rows = 1 , is_local_csv = True ): \"\"\" Load a local CSV file into a table Args: db_name (str): Database name where the CSV will be loaded table_name (str): Table name where the CSV will be loaded file_name (str): CSV file name delimiter (str): CSV delimiter character ignore_rows (str): Number of rows that will be ignored from the top write_disposition (str): Write method to add data into table (WRITE_TRUNCATE, WRITE_APPEND) is_local_csv (boolean): If the value is True, then CSV file is in local machine. If the value is False, then CSV file is in remote machine. Returns: result (str): The result of function \"\"\" result = '' if write_disposition == 'WRITE_TRUNCATE' : query = 'TRUNCATE TABLE {db_name} . {table_name} ' . format ( db_name = db_name , table_name = table_name ) self . db_cursor . execute ( query ) self . db_connection . commit () # load table if is_local_csv : sql_import_table = ( \"\"\" LOAD DATA LOCAL INFILE ' {file_name} ' INTO TABLE {db_name} . {table_name} FIELDS TERMINATED BY ' {delimiter} ' LINES TERMINATED BY ' \\\\ n' IGNORE {ignore_rows} ROWS; \"\"\" ) . format ( file_name = file_name , db_name = db_name , table_name = table_name , delimiter = delimiter , ignore_rows = ignore_rows ) else : sql_import_table = ( \"\"\" LOAD DATA INFILE ' {file_name} ' INTO TABLE {db_name} . {table_name} FIELDS TERMINATED BY ' {delimiter} ' LINES TERMINATED BY ' \\\\ n' IGNORE {ignore_rows} ROWS; \"\"\" ) . format ( file_name = file_name , db_name = db_name , table_name = table_name , delimiter = delimiter , ignore_rows = ignore_rows ) self . db_cursor . execute ( sql_import_table ) self . db_connection . commit () result = 'OK' return result __init__ ( host , port , user , password , allow_local_infile = False ) Instantiate a DB connector. Parameters: Name Type Description Default host str Database host required port str Database port required user str Username required password str Password required allow_local_infile boolean Local infile is allowed when the value is True False Returns: Name Type Description result str Value is 'OK' when successful Source code in honeydew/mysql.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def __init__ ( self , host , port , user , password , allow_local_infile = False ): \"\"\"Instantiate a DB connector. Args: host (str): Database host port (str): Database port user (str): Username password (str): Password allow_local_infile (boolean): Local infile is allowed when the value is True Returns: result (str): Value is 'OK' when successful \"\"\" self . host = host self . port = port self . user = user self . password = password self . allow_local_infile = allow_local_infile self . db_connection = sql . connect ( host = self . host , port = self . port , user = self . user , password = self . password , ssl_disabled = True , autocommit = True , allow_local_infile = True ) self . db_cursor = self . db_connection . cursor () if allow_local_infile : query_str = \"\"\"SET GLOBAL local_infile=1\"\"\" self . db_cursor . execute ( query_str ) load_csv ( db_name , table_name , file_name , write_disposition , delimiter = ',' , ignore_rows = 1 , is_local_csv = True ) Load a local CSV file into a table Parameters: Name Type Description Default db_name str Database name where the CSV will be loaded required table_name str Table name where the CSV will be loaded required file_name str CSV file name required delimiter str CSV delimiter character ',' ignore_rows str Number of rows that will be ignored from the top 1 write_disposition str Write method to add data into table (WRITE_TRUNCATE, WRITE_APPEND) required is_local_csv boolean If the value is True, then CSV file is in local machine. If the value is False, then CSV file is in remote machine. True Returns: Name Type Description result str The result of function Source code in honeydew/mysql.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def load_csv ( self , db_name , table_name , file_name , write_disposition , delimiter = ',' , ignore_rows = 1 , is_local_csv = True ): \"\"\" Load a local CSV file into a table Args: db_name (str): Database name where the CSV will be loaded table_name (str): Table name where the CSV will be loaded file_name (str): CSV file name delimiter (str): CSV delimiter character ignore_rows (str): Number of rows that will be ignored from the top write_disposition (str): Write method to add data into table (WRITE_TRUNCATE, WRITE_APPEND) is_local_csv (boolean): If the value is True, then CSV file is in local machine. If the value is False, then CSV file is in remote machine. Returns: result (str): The result of function \"\"\" result = '' if write_disposition == 'WRITE_TRUNCATE' : query = 'TRUNCATE TABLE {db_name} . {table_name} ' . format ( db_name = db_name , table_name = table_name ) self . db_cursor . execute ( query ) self . db_connection . commit () # load table if is_local_csv : sql_import_table = ( \"\"\" LOAD DATA LOCAL INFILE ' {file_name} ' INTO TABLE {db_name} . {table_name} FIELDS TERMINATED BY ' {delimiter} ' LINES TERMINATED BY ' \\\\ n' IGNORE {ignore_rows} ROWS; \"\"\" ) . format ( file_name = file_name , db_name = db_name , table_name = table_name , delimiter = delimiter , ignore_rows = ignore_rows ) else : sql_import_table = ( \"\"\" LOAD DATA INFILE ' {file_name} ' INTO TABLE {db_name} . {table_name} FIELDS TERMINATED BY ' {delimiter} ' LINES TERMINATED BY ' \\\\ n' IGNORE {ignore_rows} ROWS; \"\"\" ) . format ( file_name = file_name , db_name = db_name , table_name = table_name , delimiter = delimiter , ignore_rows = ignore_rows ) self . db_cursor . execute ( sql_import_table ) self . db_connection . commit () result = 'OK' return result query_to_dataframe ( query_str ) Query and store the result in a dataframe Parameters: Name Type Description Default query_str str sql query required Returns: Name Type Description result dataframe Result in a dataframe Source code in honeydew/mysql.py 53 54 55 56 57 58 59 60 61 62 63 64 def query_to_dataframe ( self , query_str ): \"\"\" Query and store the result in a dataframe Args: query_str (str): sql query Returns: result (dataframe): Result in a dataframe \"\"\" self . db_cursor . execute ( query_str ) table_rows = self . db_cursor . fetchall () df = pd . DataFrame ( table_rows , columns = self . db_cursor . column_names ) return df query_without_fetch ( query_str ) Send non DQL query Parameters: Name Type Description Default query_str str sql query required Returns: Name Type Description result str Value is 'OK' when successful Source code in honeydew/mysql.py 42 43 44 45 46 47 48 49 50 51 def query_without_fetch ( self , query_str ): \"\"\" Send non DQL query Args: query_str (str): sql query Returns: result (str): Value is 'OK' when successful \"\"\" self . db_cursor . execute ( query_str ) return 'OK'","title":"MySQL Connector"},{"location":"mysql/#mysql-connector","text":"","title":"MySQL Connector"},{"location":"mysql/#honeydew.mysql.MysqlConnector","text":"Source code in honeydew/mysql.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 class MysqlConnector : def __init__ ( self , host , port , user , password , allow_local_infile = False ): \"\"\"Instantiate a DB connector. Args: host (str): Database host port (str): Database port user (str): Username password (str): Password allow_local_infile (boolean): Local infile is allowed when the value is True Returns: result (str): Value is 'OK' when successful \"\"\" self . host = host self . port = port self . user = user self . password = password self . allow_local_infile = allow_local_infile self . db_connection = sql . connect ( host = self . host , port = self . port , user = self . user , password = self . password , ssl_disabled = True , autocommit = True , allow_local_infile = True ) self . db_cursor = self . db_connection . cursor () if allow_local_infile : query_str = \"\"\"SET GLOBAL local_infile=1\"\"\" self . db_cursor . execute ( query_str ) def query_without_fetch ( self , query_str ): \"\"\" Send non DQL query Args: query_str (str): sql query Returns: result (str): Value is 'OK' when successful \"\"\" self . db_cursor . execute ( query_str ) return 'OK' def query_to_dataframe ( self , query_str ): \"\"\" Query and store the result in a dataframe Args: query_str (str): sql query Returns: result (dataframe): Result in a dataframe \"\"\" self . db_cursor . execute ( query_str ) table_rows = self . db_cursor . fetchall () df = pd . DataFrame ( table_rows , columns = self . db_cursor . column_names ) return df def load_csv ( self , db_name , table_name , file_name , write_disposition , delimiter = ',' , ignore_rows = 1 , is_local_csv = True ): \"\"\" Load a local CSV file into a table Args: db_name (str): Database name where the CSV will be loaded table_name (str): Table name where the CSV will be loaded file_name (str): CSV file name delimiter (str): CSV delimiter character ignore_rows (str): Number of rows that will be ignored from the top write_disposition (str): Write method to add data into table (WRITE_TRUNCATE, WRITE_APPEND) is_local_csv (boolean): If the value is True, then CSV file is in local machine. If the value is False, then CSV file is in remote machine. Returns: result (str): The result of function \"\"\" result = '' if write_disposition == 'WRITE_TRUNCATE' : query = 'TRUNCATE TABLE {db_name} . {table_name} ' . format ( db_name = db_name , table_name = table_name ) self . db_cursor . execute ( query ) self . db_connection . commit () # load table if is_local_csv : sql_import_table = ( \"\"\" LOAD DATA LOCAL INFILE ' {file_name} ' INTO TABLE {db_name} . {table_name} FIELDS TERMINATED BY ' {delimiter} ' LINES TERMINATED BY ' \\\\ n' IGNORE {ignore_rows} ROWS; \"\"\" ) . format ( file_name = file_name , db_name = db_name , table_name = table_name , delimiter = delimiter , ignore_rows = ignore_rows ) else : sql_import_table = ( \"\"\" LOAD DATA INFILE ' {file_name} ' INTO TABLE {db_name} . {table_name} FIELDS TERMINATED BY ' {delimiter} ' LINES TERMINATED BY ' \\\\ n' IGNORE {ignore_rows} ROWS; \"\"\" ) . format ( file_name = file_name , db_name = db_name , table_name = table_name , delimiter = delimiter , ignore_rows = ignore_rows ) self . db_cursor . execute ( sql_import_table ) self . db_connection . commit () result = 'OK' return result","title":"MysqlConnector"},{"location":"mysql/#honeydew.mysql.MysqlConnector.__init__","text":"Instantiate a DB connector. Parameters: Name Type Description Default host str Database host required port str Database port required user str Username required password str Password required allow_local_infile boolean Local infile is allowed when the value is True False Returns: Name Type Description result str Value is 'OK' when successful Source code in honeydew/mysql.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def __init__ ( self , host , port , user , password , allow_local_infile = False ): \"\"\"Instantiate a DB connector. Args: host (str): Database host port (str): Database port user (str): Username password (str): Password allow_local_infile (boolean): Local infile is allowed when the value is True Returns: result (str): Value is 'OK' when successful \"\"\" self . host = host self . port = port self . user = user self . password = password self . allow_local_infile = allow_local_infile self . db_connection = sql . connect ( host = self . host , port = self . port , user = self . user , password = self . password , ssl_disabled = True , autocommit = True , allow_local_infile = True ) self . db_cursor = self . db_connection . cursor () if allow_local_infile : query_str = \"\"\"SET GLOBAL local_infile=1\"\"\" self . db_cursor . execute ( query_str )","title":"__init__()"},{"location":"mysql/#honeydew.mysql.MysqlConnector.load_csv","text":"Load a local CSV file into a table Parameters: Name Type Description Default db_name str Database name where the CSV will be loaded required table_name str Table name where the CSV will be loaded required file_name str CSV file name required delimiter str CSV delimiter character ',' ignore_rows str Number of rows that will be ignored from the top 1 write_disposition str Write method to add data into table (WRITE_TRUNCATE, WRITE_APPEND) required is_local_csv boolean If the value is True, then CSV file is in local machine. If the value is False, then CSV file is in remote machine. True Returns: Name Type Description result str The result of function Source code in honeydew/mysql.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def load_csv ( self , db_name , table_name , file_name , write_disposition , delimiter = ',' , ignore_rows = 1 , is_local_csv = True ): \"\"\" Load a local CSV file into a table Args: db_name (str): Database name where the CSV will be loaded table_name (str): Table name where the CSV will be loaded file_name (str): CSV file name delimiter (str): CSV delimiter character ignore_rows (str): Number of rows that will be ignored from the top write_disposition (str): Write method to add data into table (WRITE_TRUNCATE, WRITE_APPEND) is_local_csv (boolean): If the value is True, then CSV file is in local machine. If the value is False, then CSV file is in remote machine. Returns: result (str): The result of function \"\"\" result = '' if write_disposition == 'WRITE_TRUNCATE' : query = 'TRUNCATE TABLE {db_name} . {table_name} ' . format ( db_name = db_name , table_name = table_name ) self . db_cursor . execute ( query ) self . db_connection . commit () # load table if is_local_csv : sql_import_table = ( \"\"\" LOAD DATA LOCAL INFILE ' {file_name} ' INTO TABLE {db_name} . {table_name} FIELDS TERMINATED BY ' {delimiter} ' LINES TERMINATED BY ' \\\\ n' IGNORE {ignore_rows} ROWS; \"\"\" ) . format ( file_name = file_name , db_name = db_name , table_name = table_name , delimiter = delimiter , ignore_rows = ignore_rows ) else : sql_import_table = ( \"\"\" LOAD DATA INFILE ' {file_name} ' INTO TABLE {db_name} . {table_name} FIELDS TERMINATED BY ' {delimiter} ' LINES TERMINATED BY ' \\\\ n' IGNORE {ignore_rows} ROWS; \"\"\" ) . format ( file_name = file_name , db_name = db_name , table_name = table_name , delimiter = delimiter , ignore_rows = ignore_rows ) self . db_cursor . execute ( sql_import_table ) self . db_connection . commit () result = 'OK' return result","title":"load_csv()"},{"location":"mysql/#honeydew.mysql.MysqlConnector.query_to_dataframe","text":"Query and store the result in a dataframe Parameters: Name Type Description Default query_str str sql query required Returns: Name Type Description result dataframe Result in a dataframe Source code in honeydew/mysql.py 53 54 55 56 57 58 59 60 61 62 63 64 def query_to_dataframe ( self , query_str ): \"\"\" Query and store the result in a dataframe Args: query_str (str): sql query Returns: result (dataframe): Result in a dataframe \"\"\" self . db_cursor . execute ( query_str ) table_rows = self . db_cursor . fetchall () df = pd . DataFrame ( table_rows , columns = self . db_cursor . column_names ) return df","title":"query_to_dataframe()"},{"location":"mysql/#honeydew.mysql.MysqlConnector.query_without_fetch","text":"Send non DQL query Parameters: Name Type Description Default query_str str sql query required Returns: Name Type Description result str Value is 'OK' when successful Source code in honeydew/mysql.py 42 43 44 45 46 47 48 49 50 51 def query_without_fetch ( self , query_str ): \"\"\" Send non DQL query Args: query_str (str): sql query Returns: result (str): Value is 'OK' when successful \"\"\" self . db_cursor . execute ( query_str ) return 'OK'","title":"query_without_fetch()"}]}